{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2-ganimorph.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flobotics/colab/blob/master/tf2_ganimorph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3N_53xvtUQb",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/alpha/tutorials/generative/pix2pix\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/generative/dcgan\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/load_data/images#load_and_format_the_images\n",
        "\n",
        "https://www.tensorflow.org/guide/performance/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53NJP7dt9xT",
        "colab_type": "text"
      },
      "source": [
        "@inproceedings{Gokaslan2018,\n",
        "  title={Improving Shape Deformation in Unsupervised Image to Image Translation},\n",
        "  author={Aaron Gokaslan and Vivek Ramanujan and Daniel Ritchie and Kwang In Kim and James Tompkin},\n",
        "  booktitle={European Conference on Computer Vision},\n",
        "  year={2018}\n",
        "}\n",
        "\n",
        "https://github.com/brownvc/ganimorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAJoNuk5QHU2",
        "colab_type": "code",
        "outputId": "87594186-5bba-4591-cd1e-27ffb8202f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXrMqG2hntC5",
        "colab_type": "code",
        "outputId": "972d02f9-bcec-46df-dd9f-af192e057e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip3 install tensorflow-addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4FTapNoc7N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#from tensorpack import *\n",
        "#from tensorpack.utils.viz import *\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "#from tensorpack import (FeedfreeTrainerBase, QueueInput,\n",
        "#                        ModelDesc, DataFlow, StagingInputWrapper,\n",
        "#                        MultiGPUTrainerBase, LeastLoadedDeviceSetter)\n",
        "#from tensorpack.tfutils.summary import add_moving_summary\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "#import tensorpack.tfutils.symbolic_functions as symbf\n",
        "\n",
        "import cv2\n",
        "import os, sys\n",
        "import argparse\n",
        "from six.moves import map, zip\n",
        "from glob import glob\n",
        "\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#from tensorpack import *\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import IPython.display as display\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42GSJFUiZmZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpmUIEpKQSl",
        "colab_type": "text"
      },
      "source": [
        "# Input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcpyr6heK4C",
        "colab_type": "code",
        "outputId": "de2863ff-2a98-4dee-92cb-ab630dd10b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeSB753GeZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir /content/log\n",
        "#!cp -a /content/gdrive/My\\ Drive/images/model-11/* /content/log/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhjxVqpgH3M",
        "colab_type": "text"
      },
      "source": [
        "# Copy train and test data from e.g. Google-Drive to colab-notebook-machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIQ_Lw5ebCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/data/\n",
        "\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainB.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testB.tar.gz -C /content/data/\n",
        "\n",
        "!rm /content/data/testA.tar.gz\n",
        "!rm /content/data/testB.tar.gz\n",
        "!rm /content/data/trainA.tar.gz\n",
        "!rm /content/data/trainB.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMABggQHRw0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAPE = 128\n",
        "#BATCH = 16\n",
        "TEST_BATCH = 32\n",
        "NF = 64  # channel size\n",
        "\n",
        "BATCH_SIZE  = 2\n",
        "#FLAGS.batch_size = BATCH\n",
        "#FLAGS.prefetch_buffer_size = BATCH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnqVhD777SF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_image(path):\n",
        "  print_debug = 1\n",
        "  \n",
        "  image = tf.io.read_file(path)\n",
        "  \n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  if print_debug: print(\"original image-shape:{}\".format(image.shape))\n",
        "  if print_debug: print(\"original image-dtype:{}\".format(image.dtype))\n",
        "  \n",
        "  image = tf.image.resize(image, [SHAPE, SHAPE])\n",
        "  image /= 255.0  # normalize to [0,1] range\n",
        "  image = tf.transpose(image, perm=[2, 0, 1]) #seemed to break something, transpose ZipDataset\n",
        "\n",
        "  if print_debug: print(\"preprocessed image-shape:{}\".format(image.shape))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-min:{}\".format(image.numpy().min()))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-max:{}\".format(image.numpy().max()))\n",
        "  \n",
        "  return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPreL8O1EP3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_image_paths(dir):\n",
        "  print_debug = 0\n",
        "  \n",
        "  data_root = pathlib.Path(dir)\n",
        "  if print_debug: print(\"image-dir:{}\".format(data_root))\n",
        "  \n",
        "  all_image_paths = list(data_root.glob('*'))\n",
        "  all_image_paths = [str(path) for path in all_image_paths]\n",
        "  random.shuffle(all_image_paths)\n",
        "\n",
        "  if print_debug: print(\"image-count:{}\".format(len(all_image_paths)))\n",
        "  \n",
        "  if print_debug:\n",
        "    print(\"3 Pictures from this set\")\n",
        "    for n in range(3):\n",
        "      image_path = random.choice(all_image_paths)\n",
        "      display.display(display.Image(image_path))\n",
        "    \n",
        "  return all_image_paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDEdSKrLXNF",
        "colab_type": "text"
      },
      "source": [
        "# Build Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7NAZUxrRZr4",
        "colab_type": "code",
        "outputId": "80e32e05-f6a1-41a2-aa18-53631560f45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "print_debug = 1\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "trainA_dir = \"/content/data/trainA\"\n",
        "trainB_dir = \"/content/data/trainB\"\n",
        "testA_dir = \"/content/data/testA\"\n",
        "testB_dir = \"/content/data/testB\"\n",
        "\n",
        "#---Create Dataset with trainA\n",
        "all_image_paths = get_all_image_paths(trainA_dir)\n",
        "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "image_ds_trainA = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "if print_debug: print(\"Dataset shape/type:{}\".format(image_ds_trainA))\n",
        "\n",
        "#---Create Dataset with trainB\n",
        "all_image_paths = get_all_image_paths(trainB_dir)\n",
        "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "image_ds_trainB = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "if print_debug: print(\"Dataset shape/type:{}\".format(image_ds_trainA))\n",
        "\n",
        "  #--- Create ZipDataset\n",
        "image_ds_trainA_B = tf.data.Dataset.zip((image_ds_trainA, image_ds_trainB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_trainA_B))\n",
        "\n",
        "#---Create Dataset with testA\n",
        "all_image_paths = get_all_image_paths(testA_dir)\n",
        "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "image_ds_testA = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "if print_debug: print(\"Dataset shape/type:{}\".format(image_ds_testA))\n",
        "\n",
        "#---Create Dataset with testB\n",
        "all_image_paths = get_all_image_paths(testB_dir)\n",
        "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "image_ds_testB = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "if print_debug: print(\"Dataset shape/type:{}\".format(image_ds_testB))\n",
        "\n",
        "#--- Create ZipDataset\n",
        "image_ds_testA_B = tf.data.Dataset.zip((image_ds_testA, image_ds_testB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_testA_B))\n",
        "#print(\"len:{}\".format(image_ds_testA_B.))\n",
        "\n",
        "#------\n",
        "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
        "# completely shuffled.\n",
        "image_ds_trainA_B = image_ds_trainA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainA_B = image_ds_trainA_B.repeat()\n",
        "image_ds_trainA_B = image_ds_trainA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_trainA_B = image_ds_trainA_B.batch(BATCH_SIZE)\n",
        "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
        "image_ds_trainA_B = image_ds_trainA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_trainA_B shape/type {}\".format(image_ds_trainA_B))\n",
        "\n",
        "#test-images ds\n",
        "image_ds_testA_B = image_ds_testA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.repeat()\n",
        "image_ds_testA_B = image_ds_testA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_testA_B = image_ds_testA_B.batch(BATCH_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_testA_B shape/type {}\".format(image_ds_testA_B))\n",
        "\n",
        "  \n",
        "#for input_image, target in image_ds_testA_B:\n",
        "#  print(\"out:{} {}\".format(input_image, target))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original image-shape:(None, None, 3)\n",
            "original image-dtype:<dtype: 'uint8'>\n",
            "preprocessed image-shape:(3, 128, 128)\n",
            "Dataset shape/type:<ParallelMapDataset shapes: (3, 128, 128), types: tf.float32>\n",
            "original image-shape:(None, None, 3)\n",
            "original image-dtype:<dtype: 'uint8'>\n",
            "preprocessed image-shape:(3, 128, 128)\n",
            "Dataset shape/type:<ParallelMapDataset shapes: (3, 128, 128), types: tf.float32>\n",
            "zipDataset  shapes/types:<ZipDataset shapes: ((3, 128, 128), (3, 128, 128)), types: (tf.float32, tf.float32)>\n",
            "original image-shape:(None, None, 3)\n",
            "original image-dtype:<dtype: 'uint8'>\n",
            "preprocessed image-shape:(3, 128, 128)\n",
            "Dataset shape/type:<ParallelMapDataset shapes: (3, 128, 128), types: tf.float32>\n",
            "original image-shape:(None, None, 3)\n",
            "original image-dtype:<dtype: 'uint8'>\n",
            "preprocessed image-shape:(3, 128, 128)\n",
            "Dataset shape/type:<ParallelMapDataset shapes: (3, 128, 128), types: tf.float32>\n",
            "zipDataset  shapes/types:<ZipDataset shapes: ((3, 128, 128), (3, 128, 128)), types: (tf.float32, tf.float32)>\n",
            "image_ds_trainA_B shape/type <PrefetchDataset shapes: ((None, 3, 128, 128), (None, 3, 128, 128)), types: (tf.float32, tf.float32)>\n",
            "image_ds_testA_B shape/type <PrefetchDataset shapes: ((None, 3, 128, 128), (None, 3, 128, 128)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNnanKKTMCo",
        "colab_type": "text"
      },
      "source": [
        "# build the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLpSzPzunVqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def INReLU(self,x, name=None):\n",
        "  x = tfa.layers.InstanceNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTurwOdnnZhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def INLReLU(x, name=None):\n",
        "  #x = tfa.layers.InstanceNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONyQzQ6ndz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BNLReLU(x, name):\n",
        "    x = BatchNorm('bn', x)\n",
        "    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHpB6pfI-Je1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = \"channels_first\"\n",
        "debug_show_shapes = 0\n",
        "res_input = tf.Variable([3, 128, 128])\n",
        "res_l = tf.Variable([3, 128, 128])\n",
        "\n",
        "def build_res_block(x, name, chan, first=False):\n",
        "  #input = x\n",
        "  print(\"x-shape:{}\".format(x))\n",
        "  res_input = x\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(x)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  #l = tf.concat([l, input], axis=1)\n",
        "  l = tf.concat([l, res_input], axis=1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"layer1: {}\".format(l.shape))\n",
        "  \n",
        "  return l\n",
        "\n",
        "def res_group(input, name, depth, channels):\n",
        "  res_l = input\n",
        "  for k in range(depth):\n",
        "    res_l = build_res_block(res_l, name + ('/res%d' % k), channels,\n",
        "                           first=(k==0))\n",
        "    \n",
        "  return res_l\n",
        "  \n",
        "#def res_group(input, name, depth, channels):\n",
        "#  l = input\n",
        "#  for k in range(depth):\n",
        "#    l = build_res_block(l, name + ('/res%d' % k), channels,\n",
        "#            first=(k==0))\n",
        "#  return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv6fy9A-qOz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "debug_show_shapes = 1\n",
        "subDepth = 3\n",
        "df = \"channels_first\"\n",
        "inputs = tf.keras.Input(shape=(3, 128, 128)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPs8FP-Hle1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "\n",
        "   # Returns a placeholder tensor\n",
        "  \n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  conv0 = layers.Conv2D(filters=NF, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv0: {}\".format(conv0.shape))\n",
        "  \n",
        "  conv1 = layers.Conv2D(filters=NF*2,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(conv0)\n",
        "\n",
        "  if debug_show_shapes: print(\"conv1: {}\".format(conv1.shape))\n",
        "  \n",
        "  #-------------------------\n",
        "  layer1 = res_group(conv1, 'layer1', subDepth, NF*2)\n",
        "  \n",
        "  #-----------------\n",
        "  conv2 = layers.Conv2D(filters=NF*4,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv2: {}\".format(conv2.shape))\n",
        "  #----------------\n",
        "  \n",
        "  layer2 = res_group(conv2, 'layer2', subDepth, NF*4)\n",
        "  \n",
        "  #-----------------\n",
        "  conv3 = layers.Conv2D(filters=NF*8,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv3: {}\".format(conv3.shape))\n",
        "  #---------------\n",
        "  l = res_group(conv3, 'layer3', subDepth, NF*8)\n",
        "  \n",
        "  #--------------\n",
        "  \n",
        "  deconv0 = layers.Conv2DTranspose(filters=NF*4,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv0: {}\".format(deconv0.shape))\n",
        "  #-----------------\n",
        "  up1 = tf.concat([deconv0, layer2], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_2 = res_group(up1, 'blayer2', subDepth, NF * 4)\n",
        "  \n",
        "  #----------\n",
        "  deconv1 = layers.Conv2DTranspose(filters=NF*2,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv1: {}\".format(deconv1.shape))\n",
        "  \n",
        "  #-----------\n",
        "  up2 = tf.concat([deconv1, layer1], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_1 = res_group(up2, 'blayer1', subDepth, NF * 2)\n",
        "  \n",
        "  #----------\n",
        "  deconv2 = layers.Conv2DTranspose(filters=NF*1,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv2: {}\".format(deconv2.shape))\n",
        "  #-----------\n",
        "  deconv3 = layers.Conv2DTranspose(filters=3,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False, \n",
        "                                   activation=tf.sigmoid)(deconv2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv3: {}\".format(deconv3.shape))\n",
        "  #-----------\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=deconv3)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvSfrg0mUISo",
        "colab_type": "code",
        "outputId": "c9e50ac2-618a-41dc-b14f-f2837f4b0cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2445
        }
      },
      "source": [
        "generator = Generator()\n",
        "\n",
        "\n",
        "noise = tf.random.normal([1, 3, 128, 128])\n",
        "\n",
        "i = image_ds_testA.take(1)\n",
        "print(\"i:{}\".format(list(i)))\n",
        "\n",
        "\n",
        "\n",
        "gen_output = generator(noise, training=False)\n",
        "\n",
        "gen_output = tf.transpose(gen_output, [0, 2, 3, 1])\n",
        "print(gen_output.shape)\n",
        "\n",
        "plt.imshow(gen_output[0,...])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs: (None, 3, 128, 128)\n",
            "conv0: (None, 64, 64, 64)\n",
            "conv1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"conv2d_1/re_lu/Relu:0\", shape=(None, 128, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 256, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"conv2d_4/Conv2D:0\", shape=(None, 128, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 256, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"conv2d_7/Conv2D:0\", shape=(None, 128, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 256, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "conv2: (None, 256, 16, 16)\n",
            "x-shape:Tensor(\"conv2d_11/re_lu_1/Relu:0\", shape=(None, 256, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 384, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "x-shape:Tensor(\"conv2d_14/Conv2D:0\", shape=(None, 128, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 256, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "x-shape:Tensor(\"conv2d_17/Conv2D:0\", shape=(None, 128, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 256, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "conv3: (None, 512, 8, 8)\n",
            "x-shape:Tensor(\"conv2d_21/re_lu_2/Relu:0\", shape=(None, 512, 8, 8), dtype=float32)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 640, 8, 8)\n",
            "layer1: (None, 128, 8, 8)\n",
            "x-shape:Tensor(\"conv2d_24/Conv2D:0\", shape=(None, 128, 8, 8), dtype=float32)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 256, 8, 8)\n",
            "layer1: (None, 128, 8, 8)\n",
            "x-shape:Tensor(\"conv2d_27/Conv2D:0\", shape=(None, 128, 8, 8), dtype=float32)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 128, 8, 8)\n",
            "l: (None, 256, 8, 8)\n",
            "layer1: (None, 128, 8, 8)\n",
            "deconv0: (None, 256, 16, 16)\n",
            "x-shape:Tensor(\"concat_9:0\", shape=(None, 384, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 512, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "x-shape:Tensor(\"conv2d_33/Conv2D:0\", shape=(None, 128, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 256, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "x-shape:Tensor(\"conv2d_36/Conv2D:0\", shape=(None, 128, 16, 16), dtype=float32)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 128, 16, 16)\n",
            "l: (None, 256, 16, 16)\n",
            "layer1: (None, 128, 16, 16)\n",
            "deconv1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"concat_13:0\", shape=(None, 256, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 384, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"conv2d_42/Conv2D:0\", shape=(None, 128, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 256, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "x-shape:Tensor(\"conv2d_45/Conv2D:0\", shape=(None, 128, 32, 32), dtype=float32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 128, 32, 32)\n",
            "l: (None, 256, 32, 32)\n",
            "layer1: (None, 128, 32, 32)\n",
            "deconv2: (None, 64, 64, 64)\n",
            "deconv3: (None, 3, 128, 128)\n",
            "i:[<tf.Tensor: id=1274, shape=(3, 128, 128), dtype=float32, numpy=\n",
            "array([[[0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        ...,\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804]],\n",
            "\n",
            "       [[0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        ...,\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804]],\n",
            "\n",
            "       [[0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        ...,\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804],\n",
            "        [0.94509804, 0.94509804, 0.94509804, ..., 0.94509804,\n",
            "         0.94509804, 0.94509804]]], dtype=float32)>]\n",
            "(1, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f04504b32b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwuDmsZMy9CE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGQRQzOfttj7",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvOoSsVaj03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  debug_show_shapes = 0\n",
        "  \n",
        "  subDepth = 3\n",
        "  df = \"channels_first\"\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(3, 128, 128))  # Returns a placeholder tensor\n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  l = layers.Conv2D(filters=NF*2, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "   \n",
        "  #----\n",
        "  \n",
        "  relu1 = layers.Conv2D(filters=NF*4, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu1: {}\".format(relu1.shape))\n",
        "    \n",
        "  #-----------------\n",
        "    \n",
        "  relu2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu2: {}\".format(relu2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  relu3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu3: {}\".format(relu3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "\n",
        "  \n",
        "  atrous = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=2,\n",
        "                        bias_initializer=None)(relu3)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous: {}\".format(atrous.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=4,\n",
        "                        bias_initializer=None)(atrous)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous2: {}\".format(atrous2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=8,\n",
        "                        bias_initializer=None)(atrous2)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous3: {}\".format(atrous3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  merge = tf.concat([relu3, atrous3], axis=1)\n",
        "  if debug_show_shapes: print(\"merge: {}\".format(merge.shape))\n",
        "  #-----------------\n",
        "  \n",
        "  clean = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(merge)\n",
        "  \n",
        "  if debug_show_shapes: print(\"clean: {}\".format(clean.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  lsgan = layers.Conv2D(filters=1, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=tf.identity, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(clean)\n",
        "  \n",
        "  if debug_show_shapes: print(\"lsgan: {}\".format(lsgan.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  model = tf.keras.Model(inputs=inputs, outputs=[lsgan, [relu1, relu2, relu3, atrous, atrous2, atrous3, clean]] )\n",
        "  \n",
        "  if debug_show_shapes: model.summary()\n",
        "\n",
        "  return model  #whats with the other outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qATmfX6bVDMh",
        "colab_type": "code",
        "outputId": "ab1fd76f-5450-4c19-b453-02bffc22b15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "discriminator = Discriminator()\n",
        "\n",
        "noise = tf.random.normal([1, 3, 128, 128])\n",
        "\n",
        "disc_out,a = discriminator([noise, gen_output], training=False)\n",
        "\n",
        "print(disc_out.shape)\n",
        "disc_out = tf.transpose(disc_out, [0, 2, 3, 1])\n",
        "print(disc_out.shape)\n",
        "\n",
        "#plt.imshow(disc_out[0,...])\n",
        "\n",
        "\n",
        "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 16, 16)\n",
            "(1, 16, 16, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f045027ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATMAAAD8CAYAAAAbkUOLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+Q1MAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxMw-xvK4Je",
        "colab_type": "text"
      },
      "source": [
        "# losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NCxP-CGYVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_match_loss(feats_real, feats_fake):\n",
        "  print(\"gfml-1\")\n",
        "  #print(feats_fake)\n",
        "  losses = []\n",
        "  for real, fake in zip(feats_real, feats_fake):\n",
        "      loss = tf.reduce_mean(tf.math.squared_difference(\n",
        "          tf.reduce_mean(real, 0),\n",
        "          tf.reduce_mean(fake, 0)),\n",
        "          name='mse_feat_' + real.op.name)\n",
        "      losses.append(loss)\n",
        "  ret = tf.reduce_mean(losses, name='feature_match_loss')\n",
        "  #add_moving_summary(ret)\n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_e9xIg3ClfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_value = tf.Variable(1.0, name='loss_scalar_val_',\n",
        "        trainable=False)\n",
        "\n",
        "loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_', trainable=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyHxPp0vGiCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_normalize(loss, update_condition, epsilon=1e-10):\n",
        "  print(\"loss-norm1:{}\".format(loss))\n",
        "  print(\"loss-norm2:{}\".format(update_condition))\n",
        "  \n",
        "  # Variable used for storing the scalar-value of the loss-function.\n",
        "  #loss_value = tf.Variable(1.0, name='loss_scalar_val_' + loss.op.name,\n",
        "  #      trainable=False)\n",
        "\n",
        "  #loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_' +\n",
        "  #      loss.op.name, trainable=False))\n",
        "\n",
        "  #TODO don't update if is_training\n",
        "  ma_loss_value = (\n",
        "      moving_averages.assign_moving_average(\n",
        "              loss_value_smooth, loss, 0.9999, zero_debias=False, name='loss_EMA'\n",
        "          )\n",
        "      )\n",
        "\n",
        "  #tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ma_loss_value)\n",
        "  # Expression used for either updating the scalar-value or\n",
        "  # just re-using the old value.\n",
        "  # Note that when loss_value.assign(loss) is evaluated, it\n",
        "  # first evaluates the loss-function which is a TensorFlow\n",
        "  # expression, and then assigns the resulting scalar-value to\n",
        "  # the loss_value variable.\n",
        "  print(\"upd-cond:{}\".format(update_condition))\n",
        "  print(\"ma-loss:{}\".format(ma_loss_value))\n",
        "  print(\"loss-val:{}\".format(loss_value))\n",
        "  loss_value_updated = tf.cond(update_condition,\n",
        "                               lambda: loss_value.assign(ma_loss_value),\n",
        "                               lambda: loss_value)\n",
        "\n",
        "\n",
        "  print(\"loss-upd\")\n",
        "  # Expression for the normalized loss-function.\n",
        "  loss_normalized = loss / (loss_value_updated + epsilon)\n",
        "  print(\"loss-upd-1\")\n",
        "  #add_moving_summary(tf.identity(loss_value, name='loss_scalar_' + loss.op.name))\n",
        "\n",
        "  return loss_normalized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEUBZUZrRLg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _tf_fspecial_gauss(size, sigma):\n",
        "  \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
        "  \"\"\"\n",
        "  x_data, y_data = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
        "\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "\n",
        "  x = tf.constant(x_data, dtype=tf.float32)\n",
        "  y = tf.constant(y_data, dtype=tf.float32)\n",
        "\n",
        "  g = tf.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
        "  return g / tf.reduce_sum(g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KXiarnhREpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ssim(img1, img2, cs_map=False, mean_metric=True, size=8, sigma=1.5):\n",
        "  window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
        "  K1 = 0.03\n",
        "  K2 = 0.05\n",
        "  L = 1  # depth of image (255 in case the image has a differnt scale)\n",
        "  C1 = (K1*L)**2\n",
        "  C2 = (K2*L)**2\n",
        "  mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu1_sq = mu1*mu1\n",
        "  mu2_sq = mu2*mu2\n",
        "  mu1_mu2 = mu1*mu2\n",
        "  sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
        "  sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
        "  sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
        "  sigma1_sq = tf.abs(sigma1_sq)\n",
        "  sigma2_sq = tf.abs(sigma2_sq)\n",
        "  sigma12 = tf.abs(sigma12)\n",
        "  if cs_map:\n",
        "\n",
        "      value = (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2)),\n",
        "              (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
        "  else:\n",
        "      value = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M2b0P0kQ9i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ms_ssim(img1, img2, mean_metric=True, level=5):\n",
        "  #From NCHW to NHWC\n",
        "  img1 = tf.transpose(img1, [0, 2, 3, 1])\n",
        "  img2 = tf.transpose(img2, [0, 2, 3, 1])\n",
        "\n",
        "  weight = tf.constant([0.0448, 0.2856, 0.3001, 0.2363, 0.1333], dtype=tf.float32)\n",
        "  mssim = []\n",
        "  mcs = []\n",
        "  for l in range(level):\n",
        "      ssim_map, cs_map = tf_ssim(img1, img2, cs_map=True, mean_metric=False)\n",
        "      mssim.append(tf.reduce_mean(ssim_map))\n",
        "      mcs.append(tf.reduce_mean(cs_map))\n",
        "      filtered_im1 = tf.nn.avg_pool(img1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      filtered_im2 = tf.nn.avg_pool(img2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      img1 = filtered_im1\n",
        "      img2 = filtered_im2\n",
        "\n",
        "  # list to tensor of dim D+1\n",
        "  mssim = tf.stack(mssim, axis=0)\n",
        "  mcs = tf.stack(mcs, axis=0)\n",
        "\n",
        "  value = (tf.reduce_prod(mcs[0:level-1]**weight[0:level-1])*\n",
        "                          (mssim[level-1]**weight[level-1]))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF5iWLVAQrh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_dssim(img1, img2):\n",
        "  img1 = tf.unstack(tf.expand_dims(img1, axis=2), axis=1)\n",
        "  img2 = tf.unstack(tf.expand_dims(img2, axis=2), axis=1)\n",
        "  value = tf.stack([tf_ms_ssim(i1, i2) for i1, i2 in zip(img1, img2)], axis=0)\n",
        "  return tf.subtract(1.0, tf.reduce_sum(value)/3, name='DSSIM_loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErsvNLhPTGqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_losses(logits_real, logits_fake):\n",
        "  #Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n",
        "  #D and G play two-player minimax game with value function V(G,D)\n",
        "  #  min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n",
        "  #Args:\n",
        "  #    logits_real (tf.Tensor): discrim logits from real samples\n",
        "  #    logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n",
        "\n",
        "  with tf.name_scope(\"GAN_loss\"):\n",
        "    score_real = tf.sigmoid(logits_real)\n",
        "    score_fake = tf.sigmoid(logits_fake)\n",
        "    #tf.summary.histogram('score-real', score_real)\n",
        "    #tf.summary.histogram('score-fake', score_fake)\n",
        "\n",
        "    with tf.name_scope(\"discrim\"):\n",
        "        d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n",
        "        d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n",
        "\n",
        "        d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n",
        "        d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n",
        "\n",
        "        d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n",
        "        d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n",
        "\n",
        "    with tf.name_scope(\"gen\"):\n",
        "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n",
        "        g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n",
        "\n",
        "    #add_moving_summary(g_loss, d_loss, d_accuracy, g_accuracy)\n",
        "    \n",
        "    return g_loss, d_loss, d_accuracy, g_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpT89fEdmnHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "  \n",
        "  return d_loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KepfzD6nm2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake):\n",
        "  \n",
        "  recon_loss_A = tf_dssim(A, ABA)\n",
        "  if print_debug: print(\"recon_loss_A-shape: {}\".format(recon_loss_A.shape))\n",
        "  if print_debug: print(\"A-shape: {}\".format(A.shape))\n",
        "  if print_debug: print(\"ABA-shape: {}\".format(ABA.shape))\n",
        "  recon_loss_A_l = tf.compat.v1.losses.absolute_difference(A,\n",
        "                                            ABA,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_A_l-shape: {}\".format(recon_loss_A_l.shape))\n",
        "\n",
        "  # gan loss\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(A_dis_real, A_dis_fake)\n",
        "  G_loss_A = g_loss\n",
        "  D_loss_A = d_loss\n",
        "  # feature matching loss\n",
        "  if print_debug: print(A_feats_fake)\n",
        "  fm_loss_A = get_feature_match_loss(A_feats_real, A_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  recon_loss_B = tf_dssim(B, BAB)\n",
        "  if print_debug: print(\"recon_loss_B-shape: {}\".format(recon_loss_B.shape))\n",
        "\n",
        "  recon_loss_B_l = tf.compat.v1.losses.absolute_difference(B,\n",
        "                                            BAB,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_B_l-shape: {}\".format(recon_loss_B_l.shape))\n",
        "\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(B_dis_real, B_dis_fake)\n",
        "  G_loss_B = g_loss\n",
        "  D_loss_B = d_loss# + grad_penalty_B\n",
        "  fm_loss_B = get_feature_match_loss(B_feats_real, B_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  global_step = np.int64(1)   #get_global_step_var()\n",
        "  rate = 0.01  #tf.train.piecewise_constant(global_step, [np.int64(15000), np.int64(25000), np.int64(50000), np.int64(100000)], [0.01, 0.10, 0.15, 0.20, 0.25])\n",
        "  #rate = tf.identity(rate, name='rate')   # mitigate a TF bug\n",
        "  loss_update = tf.logical_or(tf.equal(global_step, tf.constant(36,\n",
        "      dtype=np.int64)), tf.equal(global_step % 90, tf.constant(0, dtype=np.int64)))\n",
        "  rate = tf.constant(0.33, np.float32, name='static_rate')\n",
        "\n",
        "\n",
        "  g_loss = tf.add_n([\n",
        "      (loss_normalize(G_loss_A + G_loss_B, loss_update) * 0.7 +\n",
        "      loss_normalize(fm_loss_A + fm_loss_B, loss_update) * 0.3) * (1 - rate),\n",
        "      (loss_normalize((recon_loss_A + recon_loss_B), loss_update) *\n",
        "          0.7 +\n",
        "  loss_normalize((recon_loss_A_l + recon_loss_B_l),\n",
        "              loss_update) * 0.3) * rate], name='G_loss_total')\n",
        "\n",
        "\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "\n",
        "  #@TODO what todo? collect_variables('gen', 'discrim')\n",
        "\n",
        "  g_loss = g_loss\n",
        "  d_loss = d_loss \n",
        "\n",
        "  \n",
        "  return g_loss, d_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD46Ue449oqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss1(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake):\n",
        "  \n",
        "  recon_loss_A = tf_dssim(A, ABA)\n",
        "  \n",
        "  recon_loss_A_l = tf.compat.v1.losses.absolute_difference(A,\n",
        "                                            ABA,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  \n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(A_dis_real, A_dis_fake)\n",
        "  \n",
        "  G_loss_A = g_loss\n",
        "  D_loss_A = d_loss\n",
        "  \n",
        "  fm_loss_A = get_feature_match_loss(A_feats_real, A_feats_fake)\n",
        "  \n",
        "  #-------------------\n",
        "\n",
        "  recon_loss_B = tf_dssim(B, BAB)\n",
        "  if print_debug: print(\"recon_loss_B-shape: {}\".format(recon_loss_B.shape))\n",
        "\n",
        "  recon_loss_B_l = tf.compat.v1.losses.absolute_difference(B,\n",
        "                                            BAB,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_B_l-shape: {}\".format(recon_loss_B_l.shape))\n",
        "\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(B_dis_real, B_dis_fake)\n",
        "  G_loss_B = g_loss\n",
        "  D_loss_B = d_loss# + grad_penalty_B\n",
        "  fm_loss_B = get_feature_match_loss(B_feats_real, B_feats_fake)\n",
        "  #-------------------\n",
        "  \n",
        "  global_step = np.int64(1)\n",
        "  rate = 0.01\n",
        "  \n",
        "  loss_update = tf.logical_or(tf.equal(global_step, tf.constant(36,\n",
        "      dtype=np.int64)), tf.equal(global_step % 90, tf.constant(0, dtype=np.int64)))\n",
        "  \n",
        "  rate = tf.constant(0.33, np.float32, name='static_rate')\n",
        "  \n",
        "  g_loss = tf.add_n([\n",
        "      (loss_normalize(G_loss_A + G_loss_B, loss_update) * 0.7 +\n",
        "      loss_normalize(fm_loss_A + fm_loss_B, loss_update) * 0.3) * (1 - rate),\n",
        "      (loss_normalize((recon_loss_A + recon_loss_B), loss_update) *\n",
        "          0.7 +\n",
        "  loss_normalize((recon_loss_A_l + recon_loss_B_l),\n",
        "              loss_update) * 0.3) * rate], name='G_loss_total')\n",
        "  \n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "  \n",
        "  g_loss = g_loss\n",
        "  d_loss = d_loss\n",
        "  \n",
        "  return 1,2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-9Z01QLVjF",
        "colab_type": "text"
      },
      "source": [
        "# optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk036iXixrHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lr = tf.get_variable('learning_rate', initializer=2e-4,\n",
        "#                 trainable=False)\n",
        "#        return tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHsakYaLhz9",
        "colab_type": "text"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKNKgCD5Vzey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_variables(self, g_scope='gen', d_scope='discrim'):\n",
        "  #\"\"\"\n",
        "  #Assign `self.g_vars` to the parameters under scope `g_scope`,\n",
        "  #and same with `self.d_vars`.\n",
        "  #\"\"\"\n",
        "  self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n",
        "  assert self.g_vars\n",
        "  self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n",
        "  assert self.d_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQatM82Ty-EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A = tf.Variable(1.0)\n",
        "#AB = tf.Variable([3, 128, 128])\n",
        "#AB = None\n",
        "#BA = None\n",
        "#ABA = None\n",
        "#BAB = None\n",
        "\n",
        "print_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdQWnkOGx4gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(A, B):\n",
        "#  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "#    gen_output = generator(input_image, training=True)\n",
        "#\n",
        "#    disc_real_output = discriminator([input_image, target], training=True)\n",
        "#    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "#\n",
        "#    gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "#    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "#  generator_gradients = gen_tape.gradient(gen_loss,\n",
        "#                                          generator.trainable_variables)\n",
        "#  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "#                                               discriminator.trainable_variables)\n",
        "\n",
        "#  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "#                                          generator.trainable_variables))\n",
        "#  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "#                                              discriminator.trainable_variables))\n",
        "  \n",
        "  \n",
        "  if print_debug: print(\"train step\")\n",
        "  \n",
        "  print(\"train\")\n",
        "\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    AB = generator(A, training=True)\n",
        "    BA = generator(B, training=True)\n",
        "    ABA = generator(AB, training=True)\n",
        "    BAB = generator(BA, training=True)\n",
        "    if print_debug: print(\"BAB\")\n",
        "    \n",
        "    A_dis_real, A_feats_real = discriminator(A, training=True)\n",
        "    A_dis_fake, A_feats_fake = discriminator(BA, training=True)\n",
        "    B_dis_real, B_feats_real = discriminator(B, training=True)\n",
        "    B_dis_fake, B_feats_fake = discriminator(AB, training=True)\n",
        "  \n",
        "    #------------\n",
        "    g_loss, d_loss = generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake)\n",
        "      \n",
        "      \n",
        "    \n",
        "    if print_debug: print(\"end train step\")\n",
        "    \n",
        "    #add_moving_summary(recon_loss_A, recon_loss_B, rate, g_loss, d_loss,\n",
        "    #        recon_loss_A_l, recon_loss_B_l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbP4P_KFxtph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0iKglUyVOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_image, target in dataset:\n",
        "      print(\"loop\")\n",
        "      train_step(input_image, target)\n",
        "      print(\"loop1\")\n",
        "\n",
        "    print(\"after loop1-------------\")  \n",
        "    clear_output(wait=True)\n",
        "    #for inp, tar in image_ds_testA_B.take(1):\n",
        "    #  generate_images(generator, inp, tar)\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngXLySdSydzQ",
        "colab_type": "code",
        "outputId": "d8a48093-55eb-46c9-bf23-5a49fcc845b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "EPOCHS = 2\n",
        "print(image_ds_trainA_B)\n",
        "\n",
        "dst = image_ds_trainA_B.take(3)\n",
        "\n",
        "train(dst, EPOCHS)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 2 is 2.9713478088378906 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}