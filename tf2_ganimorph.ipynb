{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2-ganimorph.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flobotics/colab/blob/master/tf2_ganimorph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3N_53xvtUQb",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/alpha/tutorials/generative/pix2pix\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/generative/dcgan\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/load_data/images#load_and_format_the_images\n",
        "\n",
        "https://www.tensorflow.org/guide/performance/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53NJP7dt9xT",
        "colab_type": "text"
      },
      "source": [
        "@inproceedings{Gokaslan2018,\n",
        "  title={Improving Shape Deformation in Unsupervised Image to Image Translation},\n",
        "  author={Aaron Gokaslan and Vivek Ramanujan and Daniel Ritchie and Kwang In Kim and James Tompkin},\n",
        "  booktitle={European Conference on Computer Vision},\n",
        "  year={2018}\n",
        "}\n",
        "\n",
        "https://github.com/brownvc/ganimorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAJoNuk5QHU2",
        "colab_type": "code",
        "outputId": "b4bfb781-bb8e-4686-850e-09e53f74f8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXrMqG2hntC5",
        "colab_type": "code",
        "outputId": "33f07188-2417-4320-d770-4d9ea353c877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip3 install tensorflow-addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4FTapNoc7N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#from tensorpack import *\n",
        "#from tensorpack.utils.viz import *\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "#from tensorpack import (FeedfreeTrainerBase, QueueInput,\n",
        "#                        ModelDesc, DataFlow, StagingInputWrapper,\n",
        "#                        MultiGPUTrainerBase, LeastLoadedDeviceSetter)\n",
        "#from tensorpack.tfutils.summary import add_moving_summary\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "#import tensorpack.tfutils.symbolic_functions as symbf\n",
        "\n",
        "import cv2\n",
        "import os, sys\n",
        "import argparse\n",
        "from six.moves import map, zip\n",
        "from glob import glob\n",
        "\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#from tensorpack import *\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import IPython.display as display\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42GSJFUiZmZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpmUIEpKQSl",
        "colab_type": "text"
      },
      "source": [
        "# Input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcpyr6heK4C",
        "colab_type": "code",
        "outputId": "7d67d151-91eb-4b3b-cc41-3823aa37ace0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeSB753GeZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir /content/log\n",
        "#!cp -a /content/gdrive/My\\ Drive/images/model-11/* /content/log/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhjxVqpgH3M",
        "colab_type": "text"
      },
      "source": [
        "# Copy train and test data from e.g. Google-Drive to colab-notebook-machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIQ_Lw5ebCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/data/\n",
        "\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainB.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testB.tar.gz -C /content/data/\n",
        "\n",
        "!rm /content/data/testA.tar.gz\n",
        "!rm /content/data/testB.tar.gz\n",
        "!rm /content/data/trainA.tar.gz\n",
        "!rm /content/data/trainB.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMABggQHRw0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAPE = 128\n",
        "#BATCH = 16\n",
        "TEST_BATCH = 32\n",
        "NF = 64  # channel size\n",
        "\n",
        "BATCH_SIZE  = 2\n",
        "#FLAGS.batch_size = BATCH\n",
        "#FLAGS.prefetch_buffer_size = BATCH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnqVhD777SF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_image(path):\n",
        "  print_debug = False\n",
        "  \n",
        "  image = tf.io.read_file(path)\n",
        "  \n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  if print_debug: print(\"original image-shape:{}\".format(image.shape))\n",
        "  if print_debug: print(\"original image-dtype:{}\".format(image.dtype))\n",
        "  \n",
        "  image = tf.image.resize(image, [SHAPE, SHAPE])\n",
        "  image /= 255.0  # normalize to [0,1] range\n",
        "  image = tf.transpose(image, perm=[2, 0, 1]) #seemed to break something, transpose ZipDataset\n",
        "\n",
        "  if print_debug: print(\"preprocessed image-shape:{}\".format(image.shape))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-min:{}\".format(image.numpy().min()))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-max:{}\".format(image.numpy().max()))\n",
        "  \n",
        "  return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDEdSKrLXNF",
        "colab_type": "text"
      },
      "source": [
        "# Build Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7NAZUxrRZr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_debug = False\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "trainA_dir = \"/content/data/trainA\"\n",
        "trainB_dir = \"/content/data/trainB\"\n",
        "testA_dir = \"/content/data/testA\"\n",
        "testB_dir = \"/content/data/testB\"\n",
        "\n",
        "image_ds_trainA = tf.data.Dataset.list_files(trainA_dir+'/*.jpg')\n",
        "image_ds_trainA = image_ds_trainA.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainA = image_ds_trainA.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "image_ds_trainB = tf.data.Dataset.list_files(trainB_dir+'/*.jpg')\n",
        "image_ds_trainB = image_ds_trainB.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainB = image_ds_trainB.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "image_ds_testA = tf.data.Dataset.list_files(testA_dir+'/*.jpg')\n",
        "image_ds_testA = image_ds_testA.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testA = image_ds_testA.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "image_ds_testB = tf.data.Dataset.list_files(testB_dir+'/*.jpg')\n",
        "image_ds_testB = image_ds_testB.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testB = image_ds_testB.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "#--- Create ZipDataset\n",
        "image_ds_trainA_B = tf.data.Dataset.zip((image_ds_trainA, image_ds_trainB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_trainA_B))\n",
        "\n",
        "#--- Create ZipDataset\n",
        "image_ds_testA_B = tf.data.Dataset.zip((image_ds_testA, image_ds_testB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_testA_B))\n",
        "#print(\"len:{}\".format(image_ds_testA_B.))\n",
        "\n",
        "#------\n",
        "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
        "# completely shuffled.\n",
        "image_ds_trainA_B = image_ds_trainA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainA_B = image_ds_trainA_B.repeat()\n",
        "image_ds_trainA_B = image_ds_trainA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_trainA_B = image_ds_trainA_B.batch(BATCH_SIZE)\n",
        "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
        "image_ds_trainA_B = image_ds_trainA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_trainA_B shape/type {}\".format(image_ds_trainA_B))\n",
        "\n",
        "#test-images ds\n",
        "image_ds_testA_B = image_ds_testA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.repeat()\n",
        "image_ds_testA_B = image_ds_testA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_testA_B = image_ds_testA_B.batch(BATCH_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_testA_B shape/type {}\".format(image_ds_testA_B))\n",
        "\n",
        "  \n",
        "#for input_image, target in image_ds_testA_B:\n",
        "#  print(\"out:{} {}\".format(input_image, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y71bQ_un8bVz",
        "colab_type": "text"
      },
      "source": [
        "# Lets display some images from the train and test dataset to see if our dataset is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFr5gKqW4RFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  show_ds = image_ds_trainA_B.take(2)\n",
        "  print(show_ds)\n",
        "\n",
        "  for i1, i2 in show_ds:\n",
        "    plt.figure()\n",
        "    i1 = tf.transpose(i1, [0,2,3,1])\n",
        "    plt.imshow(i1[0,...])\n",
        "\n",
        "    plt.figure()\n",
        "    i2 = tf.transpose(i2, [0,2,3,1])\n",
        "    plt.imshow(i2[0,...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UlpAtDA8mE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  show_ds = image_ds_testA_B.take(2)\n",
        "  print(show_ds)\n",
        "\n",
        "  for i1, i2 in show_ds:\n",
        "    plt.figure()\n",
        "    i1 = tf.transpose(i1, [0,2,3,1])\n",
        "    plt.imshow(i1[0,...])\n",
        "\n",
        "    plt.figure()\n",
        "    i2 = tf.transpose(i2, [0,2,3,1])\n",
        "    plt.imshow(i2[0,...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNnanKKTMCo",
        "colab_type": "text"
      },
      "source": [
        "# build the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLpSzPzunVqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def INReLU(self,x, name=None):\n",
        "#  x = tfa.layers.InstanceNormalization()(x)\n",
        "#  x = layers.ReLU()(x)\n",
        "#  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTurwOdnnZhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def INLReLU(x, name=None):\n",
        "  #x = tfa.layers.InstanceNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONyQzQ6ndz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def BNLReLU(x, name):\n",
        "#    x = BatchNorm('bn', x)\n",
        "#    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHpB6pfI-Je1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = \"channels_first\"\n",
        "debug_show_shapes = 0\n",
        "res_input = tf.Variable([3, 128, 128])\n",
        "res_l = tf.Variable([3, 128, 128])\n",
        "\n",
        "def build_res_block(x, name, chan, first=False):\n",
        "  #input = x\n",
        "  if debug_show_shapes: print(\"x-shape:{}\".format(x))\n",
        "  res_input = x\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(x)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  #l = tf.concat([l, input], axis=1)\n",
        "  l = tf.concat([l, res_input], axis=1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"layer1: {}\".format(l.shape))\n",
        "  \n",
        "  return l\n",
        "\n",
        "def res_group(input, name, depth, channels):\n",
        "  res_l = input\n",
        "  for k in range(depth):\n",
        "    res_l = build_res_block(res_l, name + ('/res%d' % k), channels,\n",
        "                           first=(k==0))\n",
        "    \n",
        "  return res_l\n",
        "  \n",
        "#def res_group(input, name, depth, channels):\n",
        "#  l = input\n",
        "#  for k in range(depth):\n",
        "#    l = build_res_block(l, name + ('/res%d' % k), channels,\n",
        "#            first=(k==0))\n",
        "#  return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv6fy9A-qOz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "debug_show_shapes = False\n",
        "subDepth = 3\n",
        "df = \"channels_first\"\n",
        "inputs = tf.keras.Input(shape=(3, 128, 128)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPs8FP-Hle1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "\n",
        "   # Returns a placeholder tensor\n",
        "  \n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  conv0 = layers.Conv2D(filters=NF, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv0: {}\".format(conv0.shape))\n",
        "  \n",
        "  conv1 = layers.Conv2D(filters=NF*2,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(conv0)\n",
        "\n",
        "  if debug_show_shapes: print(\"conv1: {}\".format(conv1.shape))\n",
        "  \n",
        "  #-------------------------\n",
        "  layer1 = res_group(conv1, 'layer1', subDepth, NF*2)\n",
        "  \n",
        "  #-----------------\n",
        "  conv2 = layers.Conv2D(filters=NF*4,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv2: {}\".format(conv2.shape))\n",
        "  #----------------\n",
        "  \n",
        "  layer2 = res_group(conv2, 'layer2', subDepth, NF*4)\n",
        "  \n",
        "  #-----------------\n",
        "  conv3 = layers.Conv2D(filters=NF*8,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv3: {}\".format(conv3.shape))\n",
        "  #---------------\n",
        "  l = res_group(conv3, 'layer3', subDepth, NF*8)\n",
        "  \n",
        "  #--------------\n",
        "  \n",
        "  deconv0 = layers.Conv2DTranspose(filters=NF*4,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv0: {}\".format(deconv0.shape))\n",
        "  #-----------------\n",
        "  up1 = tf.concat([deconv0, layer2], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_2 = res_group(up1, 'blayer2', subDepth, NF * 4)\n",
        "  \n",
        "  #----------\n",
        "  deconv1 = layers.Conv2DTranspose(filters=NF*2,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv1: {}\".format(deconv1.shape))\n",
        "  \n",
        "  #-----------\n",
        "  up2 = tf.concat([deconv1, layer1], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_1 = res_group(up2, 'blayer1', subDepth, NF * 2)\n",
        "  \n",
        "  #----------\n",
        "  deconv2 = layers.Conv2DTranspose(filters=NF*1,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv2: {}\".format(deconv2.shape))\n",
        "  #-----------\n",
        "  deconv3 = layers.Conv2DTranspose(filters=3,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False, \n",
        "                                   activation=tf.sigmoid)(deconv2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv3: {}\".format(deconv3.shape))\n",
        "  #-----------\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=deconv3)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvSfrg0mUISo",
        "colab_type": "code",
        "outputId": "ad58b3aa-9f80-4842-bc47-d6e51937e00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "generator = Generator()\n",
        "\n",
        "show_ds = image_ds_trainA_B.take(1)\n",
        "\n",
        "for i1, i2 in show_ds:\n",
        "  gen_output = generator(i1, training=False)\n",
        "\n",
        "gen_output = tf.transpose(gen_output, [0, 2, 3, 1])\n",
        "print(\"gen_output.shape: {}\".format(gen_output.shape))\n",
        "\n",
        "plt.imshow(gen_output[0,...])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gen_output.shape: (2, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f52c27987b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGQRQzOfttj7",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvOoSsVaj03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  debug_show_shapes = 0\n",
        "  \n",
        "  subDepth = 3\n",
        "  df = \"channels_first\"\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(3, 128, 128))  # Returns a placeholder tensor\n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  l = layers.Conv2D(filters=NF*2, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "   \n",
        "\n",
        "  #-----------------\n",
        "  \n",
        "  relu1 = layers.Conv2D(filters=NF*4, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu1: {}\".format(relu1.shape))\n",
        "    \n",
        "  #-----------------\n",
        "    \n",
        "  relu2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu2: {}\".format(relu2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  relu3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu3: {}\".format(relu3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "\n",
        "  \n",
        "  atrous = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=2,\n",
        "                        bias_initializer=None)(relu3)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous: {}\".format(atrous.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=4,\n",
        "                        bias_initializer=None)(atrous)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous2: {}\".format(atrous2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=8,\n",
        "                        bias_initializer=None)(atrous2)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous3: {}\".format(atrous3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  merge = tf.concat([relu3, atrous3], axis=1)\n",
        "  if debug_show_shapes: print(\"merge: {}\".format(merge.shape))\n",
        "  #-----------------\n",
        "  \n",
        "  clean = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(merge)\n",
        "  \n",
        "  if debug_show_shapes: print(\"clean: {}\".format(clean.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  lsgan = layers.Conv2D(filters=1, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=tf.identity, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(clean)\n",
        "  \n",
        "  if debug_show_shapes: print(\"lsgan: {}\".format(lsgan.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  model = tf.keras.Model(inputs=inputs, outputs=[lsgan, [relu1, relu2, relu3, atrous, atrous2, atrous3, clean]] )\n",
        "  \n",
        "  if debug_show_shapes: model.summary()\n",
        "\n",
        "  return model  #whats with the other outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qATmfX6bVDMh",
        "colab_type": "code",
        "outputId": "8deedbd9-f783-4685-974c-716752216e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "discriminator = Discriminator()\n",
        "\n",
        "noise = tf.random.normal([1, 3, 128, 128])\n",
        "\n",
        "disc_out,a = discriminator([noise, gen_output], training=False)\n",
        "\n",
        "print(disc_out.shape)\n",
        "disc_out = tf.transpose(disc_out, [0, 2, 3, 1])\n",
        "print(disc_out.shape)\n",
        "\n",
        "#plt.imshow(disc_out[0,...])\n",
        "\n",
        "\n",
        "plt.imshow(disc_out[0,...,-1], cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 16, 16)\n",
            "(1, 16, 16, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f509d962b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD8CAYAAAACGq0tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcXGWd7/HPtzsLazbCmmRIgKAs\ngwoxMLhcnAgERKLjMjAiUdAMLqPe61wGxIERBy+4DG645MYoKLIMsuQiGAKIOK+RQGTf0+yJgZCF\nAGFJOv27f9Rpraqu7jrnVHX1cr7v1+u8qDp1fud5urrz4znnOc/zKCIwMyuytoGugJnZQHMiNLPC\ncyI0s8JzIjSzwnMiNLPCcyI0s8JzIjSzwnMiNLPCcyI0s8Ib0crCJo4fF1Mn7ZI57oUYnTnmxdc6\nM8cASNljJo8Zlausda9tyRU3dnT2X1tXzgFEL76e73vcYes8f1r5Kpnna8w7oOr1LV35AnMYNyr7\nH+NTTz/DmrVrc/wV/8UUbR2vke7nXMOmxRExu5HyBoOWJsKpk3Zh6X/Ozxy3aMv0zDE3PbI6cwzA\nqBHZG8lfm/VXucq69OENueKO2XuHzDGvdub7l/+bjrW54j761ztljlHna7nKWv5y9t/Z5i35vo/l\nazfmimtry56b3jtlZOaYQ991eOaYaq/RxQfYNdWxP+apiQ0XOAi0NBGa2eAnoD1t3h4mUxU0dI9Q\n0mxJj0jqkHRasyplZgNHwKg2pdqGi9yJUFI7cAFwFLAvcLykfZtVMTMbGKUWoVJtqc5Xp8EkabSk\ny5LPl0qaWvbZ6cn+RyQdWRXXLukuSdc29hM31iKcCXRExOMRsQm4FJjTaIXMbICpdGmcZqt7qnQN\nppOB9RGxF3A+cF4Suy9wHLAfMBv4QXK+bp8HHmrshy1pJBFOAp4pe78i2WdmQ1iTW4RpGkxzgAuT\n11cAsyQp2X9pRLweEU8AHcn5kDQZeA+woNGfF1rwHKGkeZKWSVr2/LoX+rs4M2tQd2dJM1qEpGsw\n/fmYiOgENgA71In9NnAqpHzOp45GEuFKYErZ+8nJvgoRMT8iZkTEjB0njGugODNrjXStwaRFOLG7\noZNs8/q9dtIxwOqI+GOzztnI4zN3ANMlTaOUAI8D/qEptTKzASNgZPqRBWsiYkYfn6dpMHUfs0LS\nCGAssLaP2GOBYyUdDWwFjJH0i4g4IW2lq+VuESZN2M8CiyndsLw8Ih7Iez4zGxzUxM4SyhpMkkZR\najAtqjpmETA3ef1B4OYoLaa0CDgu6VWeBkwHbo+I0yNickRMTc53cyNJEBp8oDoirgOua+QcZjb4\npH00pp6I6JTU3WBqBxZGxAOSzgaWRcQi4CfAzyV1AOsoJTeS4y4HHgQ6gc9ERL5xqXV4ZImZVcg0\nsiSFWg2miDiz7PVrwId6iT0HOKePc98C3NJoHZ0IzaxC9+MzRdLSRPj8w0/yk0Ozdyp9/ObzM8cc\ncdRhmWMA7n7ulcwxL3fl+xoPmzY+V9xTGzZljtl6ZL7bwSdO3ypX3P0ffm/mmAPO/Fyusvbab1bm\nmH9Z/Fiusl7KOavR0ftnn3Vpxebs3/2maPyJOIlhNXwuDbcIzayHZl4aDwVOhGZWodn3CIcCJ0Iz\nqyDST6gwXDgRmlkPbhGaWaGVHqguViZ0IjSzCt0TsxaJE6GZVXBniZkZvjQ2s4KToM2J0MyKTahg\n18ZOhGZWQYL2Ue31DxxGnAjNrJJwi7A/jd9tHB84Nftg/Gd3f0fmmDFd+VaefmTNxswxe03INzHB\nblvW5orbZXz2yRoWP5V9MgkA7bBtrrgDfvyDzDFLP3xSrrI2PPXlzDEf2n1MrrImXX19rri7n305\nc0xnjr/h0nymDZJocyI0s6JTW7+v6zaoOBGaWQUJtwjNzHyPMCVJU4CLgJ2BAOZHxHeaVTEzGxiS\n3GucQSfwxYi4U9L2wB8lLYmIB5tUNzMbCAJ5rHE6EbEKWJW8fknSQ5RWoXciNBvSRFu7O0sykzQV\neAuwtMZn84B5AJPHb9+M4sysPxXwOcKG076k7YBfAV+IiBerP4+I+RExIyJm7LDdNo0WZ2b9TEki\nTLMNFw21CCWNpJQEL46IK5tTJTMbaL40TkmSKK1Q/1BE/EfzqmRmA0kS7TmXfx2qGmkRvg34KHCf\npLuTfV9KVrU3s6FKILcI04mI/6I0ma2ZDTMeWWJmxabh1RGSRksTYfv4iYx5/8mZ4349/a2ZY45/\n8NeZYwCOWvKtzDHPfeuJXGW9/L3LcsVNXnZN5pjV22SfwQdgu+OPzRXHDTdlDjn4sgW5iuq8+7eZ\nY565Nnv9AI7+95tzxV1+6mGZY/ZY8fvMMaM3Z5/lppqafGksaTbwHaAdWBAR51Z9PprSKLWDgLXA\n30fEk8lnpwMnA1uAz0XE4v4Y1eYWoZlVEk3rLJHUDlwAHA6sAO6QtKhqBNrJwPqI2EvSccB5wN9L\n2hc4DtgP2A24UdLe9MOotmLdETWzupSMLEmzpTAT6IiIxyNiE3ApMKfqmDnAhcnrK4BZyVMpc4BL\nI+L1iHgC6ABmRsSqiLgTSqPagO5Rbbk5EZpZpWwPVE+UtKxsm1d1tknAM2XvV9Azaf35mIjoBDYA\nO6SJ7WtUWxa+NDazStnuEa6JiBn9WZ3e1BvVloUToZlVUTNnqF4JTCl7PznZV+uYFZJGAGMpdZr0\nGtvsUW2+NDazCqUZqpt2j/AOYLqkaZJGUer8WFR1zCJgbvL6g8DNUVp8ZRFwnKTRkqYB04Hb+2NU\nm1uEZlZJom1Uc1JDRHRK+iywmNLjMwsj4gFJZwPLImIRpaT2c0kdwDpKyZLkuMspTe3XCXwmIrZI\nejtNHtXmRGhmVZp6aUySoK6r2ndm2evXgA/1EnsOcE7VvqaPanMiNLNKArV7qn4zKzAhT7pgZgUn\naPO6xmZWdG4R9qNXn3qa+z7zucxx7/3a+zLHbLkn3+D4HU84JXPMuB33ylVW29Jf5Ypb86bs38d7\nLvjfucra6eJz6x9Uw/UHHpo55r+uzjdRxkn7zswc86c31rw3X9f/235Urrgpo17PHLN464Myx7zY\n1vhyGJJoG1msNlKxflozq0++R2hmRecZqs3MvHhTZsl8Y8uAlRFxTONVMrOBJDX3geqhoBktws9T\nmg9sTBPOZWYDrYlD7IaKhtK+pMnAe4B8c6yb2aCktrZU23DRaNr/NnAqsH0T6mJmg4Ak2go2xC53\nSpd0DLA6Iv5Y57h53bPXrt+0OW9xZtZCam9LtQ0XjS7wfqyko4GtgDGSfhERJ5QfFBHzgfkA+44b\nEw2UZ2atUMDHZ3L/tBFxekRMjoiplOYPu7k6CZrZUCTfIzSzYlNb8XqNm/LTRsQtwC3NOJeZDbzh\n1NpLo1hp38zqk1BbsXqNW5oI20a1s92u2Z+7XnPvY5ljvnXKJZljAOYetnvmmIe+/vNcZe079ehc\ncVPqH9LDxL+dlausfz3ok7nifvWP2dfUuW/vfDOnbBy5b+aYJbc+mausm++sXoAtnddeyf7ExKL2\n7LMTbb3uT5ljanIiNLNiE/jS2MwKzWuWmFnhSTAi3wS0Q5UToZlVUJOX8xwKnAjNrJJwZ4mZFZ2c\nCM3MfGlsZsWmtsJ1lhQr7ZtZfcnjM2m2VKeTZkt6RFKHpNNqfD5a0mXJ50slTS377PRk/yOSjkx7\nzqycCM2sSvJAdZqt3plKaxpdABwF7AscL6l6KNDJwPqI2As4Hzgvid2X0sxW+wGzgR9Iak95zkyc\nCM2sUnevcZqtvplAR0Q8HhGbgEuBOVXHzAEuTF5fAcySpGT/pRHxekQ8AXQk50tzzkycCM2sSmnS\nhTRbCpOAZ8rer0j21TwmIjqBDcAOfcSmOWcm7iwxs57S9xpPlLSs7P38ZFb6IaWliXDzxs386fYV\nmeN2PmCnzDFvGjs6cwzAAZ/OPiPMAY9fnKusx771+1xx0x7ZJXPMPdd/O1dZx77te7ni9v34QZlj\nnjz1H3OVNTbH7D9fnb4hV1lf7nw4V9y77nlD5piXP5F9Bp+ua/pcQigdtaH0vcZrImJGH5+vpHLC\npMnJvlrHrJA0AhgLrK0TW++cmfjS2MwqiaZ1lgB3ANMlTZM0ilLnx6KqYxYBc5PXH6S07Eck+49L\nepWnAdOB21OeMxNfGptZBaGmzT4TEZ2SPgssBtqBhRHxgKSzgWURsQj4CfBzSR3AOkqJjeS4y4EH\ngU7gMxGxBaDWORupZ0OJUNI4Sou77w8EcFJE/KGRc5rZAGvyWOOIuA64rmrfmWWvXwM+1EvsOcA5\nac7ZiEZbhN8BfhMRH0yaqPmmGDazQcRjjVOTNBZ4J/AxgOR5nk3NqZaZDRgJjRg50LVoqUY6S6YB\nzwM/lXSXpAWStm1SvcxsIKkt3TZMNPKTjAAOBH4YEW8BNgK1xhHOk7RM0rINndkXsDGzVpMTYQYr\ngBURsTR5fwWlxFghIuZHxIyImDG2YM1ts6Eq1JZqGy5y/yQR8SzwjKTuJ0VnUermNrOhTBSuRdho\nr/E/ARcnPcaPAx9vvEpmNrBUWsCpQBpKhBFxN9DX8BozG2ICiPZijbUo1k9rZvVJw+qyN42WJsJt\ndtyeN39qVua4rffaJ3NM28grM8cAtB91SvaYDatylXXf3h/NFffKDuszx8SWP+Uq60+HTMsV95Z9\nJmaOGXFmjwEEqXRtyT6BQmzZkqus9kP/LlfcdYeNyxyz/rXsdVSzLmmdCM2s2NwiNDMbVo/GpOFE\naGY9ORGaWaHJky6YmfnS2MyKTlnWLBkWnAjNrFL3ELsCcSI0syp+fMbMjGgrVmoo1k9rZvV5iJ2Z\nGZ59xsyKzi1CMzM/R9ifXlr5AjefcXXmuDvWX5I55vT1+dZ7fnnBlzPHdM49O1dZu8w9LFfcxv/+\nXeaYdWfNy1XWpNO+litO9y/JHPP0Ly/NVdaUEz6SOealu/47V1kvvL/HsjypvOcrN2WOaWvLfnna\n8exLmWNqciI0syILRBe+R2hmhRZ0RQx0JVqqofavpP8p6QFJ90u6RNJWzaqYmQ2cSLkNF7kToaRJ\nwOeAGRGxP9AOHNesipnZwAigK9Jtw0Wjd0RHAFtLGgFsA+SbD97MBpWISLU1QtIESUskLU/+O76X\n4+YmxyyXNLds/0GS7pPUIem7StYpkPQNSQ9LulfSVZLqrpPQyLrGK4FvAk8Dq4ANEXFD3vOZ2eDQ\nwhbhacBNETEduCl5X0HSBOAs4GBgJnBWWcL8IfBJYHqyzU72LwH2j4gDgEeB0+tVpJFL4/HAHGAa\nsBuwraQTahw3T9IyScs2dOVbMMfMWihgS8qtQXOAC5PXFwLvq3HMkcCSiFgXEespJbnZknYFxkTE\nbVFqml7UHR8RN0REZxJ/GzC5XkUauTR+N/BERDwfEZuBK4FDqw+KiPkRMSMiZowt2Ky3ZkNVKy6N\ngZ0jonsJyGeBnWscMwl4puz9imTfpOR19f5qJwHX16tII4/PPA0cImkb4FVgFrCsgfOZ2SAQQFf6\nwydKKv93Pz8i5ne/kXQjsEuNuDMqyowISU3tfpF0BtAJXFzv2NyJMCKWSroCuDMp7C5gft9RZjYU\nZGjsrYmIGb2fJ97d22eSnpO0a0SsSi51V9c4bCVwWNn7ycAtyf7JVftXlp37Y8AxwKxI0XRtqNc4\nIs6KiDdGxP4R8dGIeL2R85nZ4NCizpJFQHcv8FzgmhrHLAaOkDQ+6Zc4AlicXFK/KOmQpLf4xO54\nSbOBU4FjI+KVNBUp1oBCM6srArZEpNoadC5wuKTllPoczgWQNEPSglJdYh3wVeCOZDs72QfwaWAB\n0AE8xl/uBX4f2B5YIuluST+qVxEPsTOzHloxwi4i1lLqW6jevwz4RNn7hcDCXo7bv8b+vbLWpaWJ\nsAvYlKM9fda1X8oc83rO3+TrL2SfveP2FS/mKuuYG7LPqgPQ+bsL6x9UZcJX8t2+bXviD7niVl55\nZeaY3Y7621xlbXr0rswx7SPz/emP/nG+2WeWnHFe5pixo7M/ZfGOG7fPHFOt9BzhMBo2koJbhGbW\nQ7HSoBOhmdUwnMYRp+FEaGY9FOzK2InQzCpFc3qEhxQnQjPrwZfGZlZogS+NzczoKli/sROhmfXg\nFqGZFZofqDazwouAzU2YdXUocSI0syp+fMbMCs6Xxv1su4nb8jcfm5k5buTu+2SOeeGCf8kcA7Dj\nrF7nkezVUc/+JldZR96ceZIMAF59aXrmmEsOybdezLqf/TRX3G7vOSJ7WbctzVXWmD3rLknRw+a/\n+1+5ylq/cXOuuOmbas052rdbnt8uc8zLmzLMLd2bgC1NOM1Q4hahmVVwi9DMCi+AzQUbWlJ3hmpJ\nCyWtlnR/2b5UCzOb2RAUsKUrUm3DRZqp+n/GXxZO7lZ3YWYzG5qCoCvSbcNF3UQYEbcC66p2p1mY\n2cyGqBYt8D5o5L1HmGZhZjMbgtxZkkO9hZklzQPmAUwas22jxZlZf0vuERZJ3uU8n0sWZKaPhZkB\niIj5ETEjImZM2GarnMWZWat09xqn2YaLvIkwzcLMZjYEdV8aF6mzpO6lsaRLgMOAiZJWAGdRWoj5\nckknA08BH+7PSppZC0XQNYxae2nUTYQRcXwvH/VYmNnMhr5gePUIp+GRJWbWw3C67E3DidDMKpTm\nIyzWrAstT4Rqz94/0/XyC5ljNr20MXMMgLbdPnPMyEl75irr+hEP5YpbfUD259dv3GtGrrKOefKP\nueJ+PT37LEPH3vz9XGV17ZT9+3/9F1/JVdbUXXbJFfe9icdmjjlu/20yx2w9Mm//51+06tJY0gTg\nMmAq8CTw4YhYX+O4ucCXk7f/HhEXJvsPojTybWvgOuDzEX9pykr6IvBNYMeIWNNXXRr/1sxs2GlR\nr3HdobpJsjwLOBiYCZxVNrfBD4FPAtOTbXZZ3BTgCODpNBVxIjSzCpHMUJ1ma1CaobpHAksiYl3S\nWlwCzE6eXx4TEbclrcCLquLPB06FdMvx+R6hmVVq3ciSNEN1JwHPlL1fkeyblLyu3o+kOcDKiLhH\nUqqKOBGaWYUgUyKcKGlZ2fv5ETG/+42kG4FaN1bPqCizzlDdtCRtA3yJ0mVxak6EZlYhAjZ1pu41\nXhMRvfbERUSva19Iek7SrhGxqo+huispDejoNhm4Jdk/uWr/SmBPYBrQ3RqcDNwpaWZEPNtbXXyP\n0MwqBOkmZW3C5XOaobqLgSMkjU86SY4AFieX1C9KOkSljHcicE1E3BcRO0XE1IiYSumS+cC+kiA4\nEZpZtdbNUH0ucLik5cC7k/dImiFpAUBErAO+CtyRbGcn+wA+DSwAOoDHgOvzVsSXxmZWIeM9wvzl\nRKylxlDdiFgGfKLs/UJgYS/H7V+njKlp6uJEaGYVooDzEToRmlkPToRmVmhdEbyevtd4WHAiNLMe\n3CLsR5tf2cRzdz6VOW79I9/LHPPcfc9njgF49Kp7M8ds2ZTv/57rX3gtV9zBJ/4mc8w/XH1WrrKe\n+fLJueKOveH8zDFb/urNucrSqxsyxzz13tNzlbXPg1fkilux7pXsMS9tyhyzqQmzxvgeoZkZNGMc\n8ZDiRGhmFbofqC4SJ0Izq5BxiN2wUHdkiaSFklZLur9s3zckPSzpXklXSRrXv9U0s1YpPVDdlWob\nLtIMsfsZZRMeJpYA+0fEAcCjQL47z2Y2+ETLxhoPGnUTYUTcCqyr2ndDRHQmb2+jchYIMxvCuofY\nFSkRNuMe4UmU1h2oSdI8YB7ALluNbkJxZtafIqBzGCW5NBpKhJLOADqBi3s7JpmkcT7APmPHFOvb\nNRuCWjXpwmCSOxFK+hhwDDCrfOUoMxvaIqJwvca5EqGk2ZQWRvkfEZH9kXkzG9TcIqwi6RJKU2VP\nlLSC0tJ6pwOjgSXJdNi3RcQp/VhPM2sRD7GrISKOr7H7J/1QFzMbJMKJ0MyKLAK6nAj7z1bjtmWv\nOTMzx216KfttyB3n55slZFR7unVQy23z2wX5ytqjz1nGezdmYuaQF6/7Za6idj/t7FxxneMmZQ+6\ntdeHD/q06q21Llr6dkBnR66yHtj/Q7ni/s+2nfUPqtLW8YfMMdt0bswc01NQtP5PtwjNrFLAFvca\nm1mRBRDFyoNOhGbWky+NzazY3FliZhZ+fMbMii0CtjRh7ZOhxInQzHpwi9DMCs+J0MwKLSIK11mS\nZqp+MyuYiEi1NULSBElLJC1P/ju+l+PmJscslzS3bP9Bku6T1CHpu0pmgEk++6dkXaUHJH29Xl2c\nCM2sh+hKtzXoNOCmiJgO3JS8ryBpAqUZrw4GZgJnlSXMHwKfBKYn2+wk5l3AHOBNEbEf8M16FXEi\nNLMKkQyxS7M1aA5wYfL6QuB9NY45ElgSEesiYj2lheNmS9oVGBMRtyUTQ19UFv8p4NyIeL3088Tq\nehVxIjSzSlHqLEmzNWjniFiVvH4W2LnGMZOAZ8rer0j2TUpeV+8H2Bt4h6Slkn4n6a31KtLSzpLN\nr77G83cvzxw35ejDMseMePL3mWMArh355swxB952V66ynv7rj+SK27Ax+0wmb/zAGbnK2nbrLbni\nfrrbgZlj9t8n+6w6AIf+4k3Zg5SvDbDxI8fmilu28MrMMYvX7Jk5ZlVnMxZIC7rS3/+bKGlZ2fv5\nyTpFAEi6EdilRlzFH2REhKRm9dCMACYAhwBvBS6XtEdfS4q419jMKpQmXUidk9ZExIxezxXx7t4+\nk/ScpF0jYlVyqVvrEnYlpRnyu00Gbkn2T67avzJ5vQK4Mkl8t0vqAiYCz/dWF18am1ml1l0aLwK6\ne4HnAtfUOGYxcISk8UknyRHA4uSS+kVJhyS9xSeWxV8NvAtA0t7AKGBNXxWpmwglLZS0WtL9NT77\noqSQlO+axswGpa6uSLU16FzgcEnLgXcn75E0Q9ICgIhYB3wVuCPZzk72AXwaWAB0AI8B1yf7FwJ7\nJDnrUmBuvZU201wa/wz4PqVemT+TNIVSdn46xTnMbIiICLpaMNY4ItYCs2rsXwZ8ouz9QkrJrdZx\nPaZ5j4hNwAlZ6lK3RRgRtwLranx0PqUlPYv1CLpZAbSoRTho5F3XeA6wMiLuKXuYu7dj5wHzAHbb\nbus8xZlZi0VXvqcFhqrMiVDSNsCXKF0W15V0pc8H2H+n8cPnfyFmw1WEE2EKewLTgO7W4GTgTkkz\nI+LZZlbOzFovcCKsKyLuA3bqfi/pSWBGRPTZPW1mQ0QEXZs3DXQtWirN4zOXAH8A3iBphaST+79a\nZjZgkkvjNNtwUbdFGBF9rp4dEVObVhszGxSGU5JLw0PszKyC7xH2s80bN/On21fUP7DKRQvPzxzz\ncs4pgs755bzMMXfekn0iCYA9t/3XXHFP3/JQ5phn1r6aq6xpV/0oV9xBB0+qf1CV/U46PFdZG2+9\nNnPM78+sNZqrvq3G55vU4OmD3pY55lP/3ONZ47qu2Vh3xqn6wi1CMyu8oMuJ0MyKLCLo6ixWr7ET\noZlViiC2uEVoZgXne4RmVmweYmdm5kRoZgVXmqq//+cjHEycCM2sknuNzazwws8RmlnBBfjxGTMr\nOPcam5k5EZpZ0RWws0R1lvtsbmHS88BTvXw8kTqLMLeI61HJ9ag02Ouxe0Ts2MiJJf0mOX8aayJi\ndiPlDQYtTYR9kbQsIma4Hq6H6zF06jFc1J2q38xsuHMiNLPCG0yJcP5AVyDhelRyPSq5HsPQoLlH\naGY2UAZTi9DMbEC0NBFKmi3pEUkdkk6r8floSZclny+VNLUf6jBF0m8lPSjpAUmfr3HMYZI2SLo7\n2c5sdj3KynpS0n1JOctqfC5J302+k3slHdjk8t9Q9nPeLelFSV+oOqbfvg9JCyWtlnR/2b4JkpZI\nWp78d3wvsXOTY5ZLmtsP9fiGpIeT7/0qSeN6ie3zd9iEevybpJVl3//RvcT2+e/L+hARLdmAduAx\nYA9gFHAPsG/VMZ8GfpS8Pg64rB/qsStwYPJ6e+DRGvU4DLi2Rd/Lk8DEPj4/GrgeEHAIsLSff0fP\nUnoWrSXfB/BO4EDg/rJ9XwdOS16fBpxXI24C8Hjy3/HJ6/FNrscRwIjk9Xm16pHmd9iEevwb8M8p\nfnd9/vvy1vvWyhbhTKAjIh6PiE3ApcCcqmPmABcmr68AZklSMysREasi4s7k9UvAQ0D2tSdbZw5w\nUZTcBoyTtGs/lTULeCwienvoveki4lZgXdXu8r+DC4H31Qg9ElgSEesiYj2wBMj9YG+tekTEDRHR\nmby9DZic9/yN1COlNP++rBetTISTgGfK3q+gZwL68zHJH+AGYIf+qlBy6f0WYGmNj/9G0j2Srpe0\nX3/VgdJkHzdI+qOkWosqp/nemuU44JJePmvV9wGwc0SsSl4/C+xc45hWfi8AJ1FqmddS73fYDJ9N\nLtEX9nKroNXfx7BS2M4SSdsBvwK+EBEvVn18J6XLwzcB3wOu7seqvD0iDgSOAj4j6Z39WFavJI0C\njgX+s8bHrfw+KkTpum9AH22QdAbQCVzcyyH9/Tv8IbAn8GZgFfCtJp+/8FqZCFcCU8reT0721TxG\n0ghgLLC22RWRNJJSErw4Iq6s/jwiXoyIl5PX1wEjJaUde5lJRKxM/rsauIrSJU65NN9bMxwF3BkR\nz9WoY8u+j8Rz3Zf/yX9X1zimJd+LpI8BxwAfSZJyDyl+hw2JiOciYktEdAH/t5fzt+rvZFhqZSK8\nA5guaVrS+jgOWFR1zCKgu/fvg8DNvf3x5ZXcc/wJ8FBE/Ecvx+zSfW9S0kxK31N/JORtJW3f/ZrS\nzfn7qw5bBJyY9B4fAmwou2xspuPp5bK4Vd9HmfK/g7nANTWOWQwcIWl8cql4RLKvaSTNBk4Fjo2I\nV3o5Js3vsNF6lN8Tfn8v50/z78t608qeGUo9oI9S6t06I9l3NqU/NICtKF2adQC3A3v0Qx3eTulS\n617g7mQ7GjgFOCU55rPAA5R63m4DDu2n72OPpIx7kvK6v5Pyugi4IPnO7gNm9EM9tqWU2MaW7WvJ\n90Ep+a4CNlO6r3UypfvCNwHLgRuBCcmxM4AFZbEnJX8rHcDH+6EeHZTuu3X/nXQ/0bAbcF1fv8Mm\n1+Pnye/+XkrJbdfqevT278to8caaAAAAOElEQVRbus0jS8ys8ArbWWJm1s2J0MwKz4nQzArPidDM\nCs+J0MwKz4nQzArPidDMCs+J0MwK7/8D2BpapCdESM4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxMw-xvK4Je",
        "colab_type": "text"
      },
      "source": [
        "# losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NCxP-CGYVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_match_loss(feats_real, feats_fake):\n",
        "\n",
        "  losses = []\n",
        "  for real, fake in zip(feats_real, feats_fake):\n",
        "      loss = tf.reduce_mean(tf.math.squared_difference(\n",
        "          tf.reduce_mean(real, 0),\n",
        "          tf.reduce_mean(fake, 0)),\n",
        "          name='mse_feat_' + real.op.name)\n",
        "      losses.append(loss)\n",
        "  ret = tf.reduce_mean(losses, name='feature_match_loss')\n",
        "  \n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_e9xIg3ClfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_value = tf.Variable(1.0, name='loss_scalar_val_',\n",
        "        trainable=False)\n",
        "\n",
        "loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_', trainable=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyHxPp0vGiCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_normalize(loss, update_condition, epsilon=1e-10):\n",
        "  \n",
        "  # Variable used for storing the scalar-value of the loss-function.\n",
        "  #loss_value = tf.Variable(1.0, name='loss_scalar_val_' + loss.op.name,\n",
        "  #      trainable=False)\n",
        "\n",
        "  #loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_' +\n",
        "  #      loss.op.name, trainable=False))\n",
        "\n",
        "  #TODO don't update if is_training\n",
        "  ma_loss_value = (\n",
        "      moving_averages.assign_moving_average(\n",
        "              loss_value_smooth, loss, 0.9999, zero_debias=False, name='loss_EMA'\n",
        "          )\n",
        "      )\n",
        "\n",
        "  #tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ma_loss_value)\n",
        "  # Expression used for either updating the scalar-value or\n",
        "  # just re-using the old value.\n",
        "  # Note that when loss_value.assign(loss) is evaluated, it\n",
        "  # first evaluates the loss-function which is a TensorFlow\n",
        "  # expression, and then assigns the resulting scalar-value to\n",
        "  # the loss_value variable.\n",
        "  loss_value_updated = tf.cond(update_condition,\n",
        "                               lambda: loss_value.assign(ma_loss_value),\n",
        "                               lambda: loss_value)\n",
        "\n",
        "\n",
        "  # Expression for the normalized loss-function.\n",
        "  loss_normalized = loss / (loss_value_updated + epsilon)\n",
        "\n",
        "  #add_moving_summary(tf.identity(loss_value, name='loss_scalar_' + loss.op.name))\n",
        "\n",
        "  return loss_normalized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEUBZUZrRLg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _tf_fspecial_gauss(size, sigma):\n",
        "  \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
        "  \"\"\"\n",
        "  x_data, y_data = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
        "\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "\n",
        "  x = tf.constant(x_data, dtype=tf.float32)\n",
        "  y = tf.constant(y_data, dtype=tf.float32)\n",
        "\n",
        "  g = tf.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
        "  return g / tf.reduce_sum(g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KXiarnhREpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ssim(img1, img2, cs_map=False, mean_metric=True, size=8, sigma=1.5):\n",
        "  window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
        "  K1 = 0.03\n",
        "  K2 = 0.05\n",
        "  L = 1  # depth of image (255 in case the image has a differnt scale)\n",
        "  C1 = (K1*L)**2\n",
        "  C2 = (K2*L)**2\n",
        "  mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu1_sq = mu1*mu1\n",
        "  mu2_sq = mu2*mu2\n",
        "  mu1_mu2 = mu1*mu2\n",
        "  sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
        "  sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
        "  sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
        "  sigma1_sq = tf.abs(sigma1_sq)\n",
        "  sigma2_sq = tf.abs(sigma2_sq)\n",
        "  sigma12 = tf.abs(sigma12)\n",
        "  if cs_map:\n",
        "\n",
        "      value = (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2)),\n",
        "              (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
        "  else:\n",
        "      value = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M2b0P0kQ9i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ms_ssim(img1, img2, mean_metric=True, level=5):\n",
        "  #From NCHW to NHWC\n",
        "  img1 = tf.transpose(img1, [0, 2, 3, 1])\n",
        "  img2 = tf.transpose(img2, [0, 2, 3, 1])\n",
        "\n",
        "  weight = tf.constant([0.0448, 0.2856, 0.3001, 0.2363, 0.1333], dtype=tf.float32)\n",
        "  mssim = []\n",
        "  mcs = []\n",
        "  for l in range(level):\n",
        "      ssim_map, cs_map = tf_ssim(img1, img2, cs_map=True, mean_metric=False)\n",
        "      mssim.append(tf.reduce_mean(ssim_map))\n",
        "      mcs.append(tf.reduce_mean(cs_map))\n",
        "      filtered_im1 = tf.nn.avg_pool(img1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      filtered_im2 = tf.nn.avg_pool(img2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      img1 = filtered_im1\n",
        "      img2 = filtered_im2\n",
        "\n",
        "  # list to tensor of dim D+1\n",
        "  mssim = tf.stack(mssim, axis=0)\n",
        "  mcs = tf.stack(mcs, axis=0)\n",
        "\n",
        "  value = (tf.reduce_prod(mcs[0:level-1]**weight[0:level-1])*\n",
        "                          (mssim[level-1]**weight[level-1]))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF5iWLVAQrh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_dssim(img1, img2):\n",
        "  img1 = tf.unstack(tf.expand_dims(img1, axis=2), axis=1)\n",
        "  img2 = tf.unstack(tf.expand_dims(img2, axis=2), axis=1)\n",
        "  value = tf.stack([tf_ms_ssim(i1, i2) for i1, i2 in zip(img1, img2)], axis=0)\n",
        "  return tf.subtract(1.0, tf.reduce_sum(value)/3, name='DSSIM_loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErsvNLhPTGqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_losses(logits_real, logits_fake):\n",
        "  #Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n",
        "  #D and G play two-player minimax game with value function V(G,D)\n",
        "  #  min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n",
        "  #Args:\n",
        "  #    logits_real (tf.Tensor): discrim logits from real samples\n",
        "  #    logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n",
        "\n",
        "  with tf.name_scope(\"GAN_loss\"):\n",
        "    score_real = tf.sigmoid(logits_real)\n",
        "    score_fake = tf.sigmoid(logits_fake)\n",
        "    #tf.summary.histogram('score-real', score_real)\n",
        "    #tf.summary.histogram('score-fake', score_fake)\n",
        "\n",
        "    with tf.name_scope(\"discrim\"):\n",
        "        d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n",
        "        d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n",
        "\n",
        "        d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n",
        "        d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n",
        "\n",
        "        d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n",
        "        d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n",
        "\n",
        "    with tf.name_scope(\"gen\"):\n",
        "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n",
        "        g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n",
        "\n",
        "    #add_moving_summary(g_loss, d_loss, d_accuracy, g_accuracy)\n",
        "    \n",
        "    return g_loss, d_loss, d_accuracy, g_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpT89fEdmnHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "  \n",
        "  return d_loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KepfzD6nm2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake):\n",
        "  print_debug = False\n",
        "  \n",
        "  recon_loss_A = tf_dssim(A, ABA)\n",
        "  if print_debug: print(\"recon_loss_A-shape: {}\".format(recon_loss_A.shape))\n",
        "  if print_debug: print(\"A-shape: {}\".format(A.shape))\n",
        "  if print_debug: print(\"ABA-shape: {}\".format(ABA.shape))\n",
        "  recon_loss_A_l = tf.compat.v1.losses.absolute_difference(A,\n",
        "                                            ABA,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_A_l-shape: {}\".format(recon_loss_A_l.shape))\n",
        "\n",
        "  # gan loss\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(A_dis_real, A_dis_fake)\n",
        "  G_loss_A = g_loss\n",
        "  D_loss_A = d_loss\n",
        "  # feature matching loss\n",
        "  if print_debug: print(A_feats_fake)\n",
        "  fm_loss_A = get_feature_match_loss(A_feats_real, A_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  recon_loss_B = tf_dssim(B, BAB)\n",
        "  if print_debug: print(\"recon_loss_B-shape: {}\".format(recon_loss_B.shape))\n",
        "\n",
        "  recon_loss_B_l = tf.compat.v1.losses.absolute_difference(B,\n",
        "                                            BAB,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_B_l-shape: {}\".format(recon_loss_B_l.shape))\n",
        "\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(B_dis_real, B_dis_fake)\n",
        "  G_loss_B = g_loss\n",
        "  D_loss_B = d_loss# + grad_penalty_B\n",
        "  fm_loss_B = get_feature_match_loss(B_feats_real, B_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  global_step = np.int64(1)   #get_global_step_var()\n",
        "  rate = 0.01  #tf.train.piecewise_constant(global_step, [np.int64(15000), np.int64(25000), np.int64(50000), np.int64(100000)], [0.01, 0.10, 0.15, 0.20, 0.25])\n",
        "  #rate = tf.identity(rate, name='rate')   # mitigate a TF bug\n",
        "  loss_update = tf.logical_or(tf.equal(global_step, tf.constant(36,\n",
        "      dtype=np.int64)), tf.equal(global_step % 90, tf.constant(0, dtype=np.int64)))\n",
        "  rate = tf.constant(0.33, np.float32, name='static_rate')\n",
        "\n",
        "\n",
        "  g_loss = tf.add_n([\n",
        "      (loss_normalize(G_loss_A + G_loss_B, loss_update) * 0.7 +\n",
        "      loss_normalize(fm_loss_A + fm_loss_B, loss_update) * 0.3) * (1 - rate),\n",
        "      (loss_normalize((recon_loss_A + recon_loss_B), loss_update) *\n",
        "          0.7 +\n",
        "  loss_normalize((recon_loss_A_l + recon_loss_B_l),\n",
        "              loss_update) * 0.3) * rate], name='G_loss_total')\n",
        "\n",
        "\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "\n",
        "  #@TODO what todo? collect_variables('gen', 'discrim')\n",
        "\n",
        "  g_loss = g_loss\n",
        "  d_loss = d_loss \n",
        "\n",
        "  \n",
        "  return g_loss, d_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-9Z01QLVjF",
        "colab_type": "text"
      },
      "source": [
        "# optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk036iXixrHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lr = tf.get_variable('learning_rate', initializer=2e-4,\n",
        "#                 trainable=False)\n",
        "#        return tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHsakYaLhz9",
        "colab_type": "text"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKNKgCD5Vzey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_variables(self, g_scope='gen', d_scope='discrim'):\n",
        "  #\"\"\"\n",
        "  #Assign `self.g_vars` to the parameters under scope `g_scope`,\n",
        "  #and same with `self.d_vars`.\n",
        "  #\"\"\"\n",
        "  self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n",
        "  assert self.g_vars\n",
        "  self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n",
        "  assert self.d_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQatM82Ty-EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A = tf.Variable(1.0)\n",
        "#AB = tf.Variable([3, 128, 128])\n",
        "#AB = None\n",
        "#BA = None\n",
        "#ABA = None\n",
        "#BAB = None\n",
        "\n",
        "print_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdQWnkOGx4gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(A, B):\n",
        "#  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "#    gen_output = generator(input_image, training=True)\n",
        "#\n",
        "#    disc_real_output = discriminator([input_image, target], training=True)\n",
        "#    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "#\n",
        "#    gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "#    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "#  generator_gradients = gen_tape.gradient(gen_loss,\n",
        "#                                          generator.trainable_variables)\n",
        "#  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "#                                               discriminator.trainable_variables)\n",
        "\n",
        "#  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "#                                          generator.trainable_variables))\n",
        "#  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "#                                              discriminator.trainable_variables))\n",
        "  \n",
        "  \n",
        "  if print_debug: print(\"train step\")\n",
        "  \n",
        "  print(\"train\")\n",
        "\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    AB = generator(A, training=True)\n",
        "    BA = generator(B, training=True)\n",
        "    ABA = generator(AB, training=True)\n",
        "    BAB = generator(BA, training=True)\n",
        "    if print_debug: print(\"BAB\")\n",
        "    \n",
        "    A_dis_real, A_feats_real = discriminator(A, training=True)\n",
        "    A_dis_fake, A_feats_fake = discriminator(BA, training=True)\n",
        "    B_dis_real, B_feats_real = discriminator(B, training=True)\n",
        "    B_dis_fake, B_feats_fake = discriminator(AB, training=True)\n",
        "  \n",
        "    #------------\n",
        "    g_loss, d_loss = generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake)\n",
        "      \n",
        "  gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))  \n",
        "    \n",
        "  if print_debug: print(\"end train step\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbP4P_KFxtph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BocGsTuOL8BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = tf.random.normal([1, 3, 128, 128])\n",
        "dstest = image_ds_testA_B.take(1)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  print(\"generate_and_save_images\")\n",
        "  \n",
        "  for input_image, target in dstest:\n",
        "  \n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(input_image, training=False)\n",
        "\n",
        "  predictions = tf.transpose(predictions, [0, 2, 3, 1])\n",
        "  print(\"predications shape:{}\".format(predictions.shape))\n",
        "  #plt.imshow(predictions[0,...])\n",
        "  \n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i,...])\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0iKglUyVOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_image, target in dataset:\n",
        "      train_step(input_image, target)\n",
        "      \n",
        "    clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             1,\n",
        "                             seed)\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngXLySdSydzQ",
        "colab_type": "code",
        "outputId": "48f34459-6e5c-482e-b05e-bdc591ea35d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "EPOCHS = 200\n",
        "print(image_ds_trainA_B)\n",
        "\n",
        "dst = image_ds_trainA_B.take(100)\n",
        "\n",
        "train(dst, EPOCHS)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generate_and_save_images\n",
            "predications shape:(2, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJEAAABSCAYAAABUpDY5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/xdCW4PlyDWr0wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 2 is 15.625678539276123 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3f6aa120c5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_ds_trainA_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-6cc85132cb51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \"\"\"\n\u001b[1;32m    573\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 574\u001b[0;31m         (t for t in nest.flatten((args, kwargs))\n\u001b[0m\u001b[1;32m    575\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    576\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}