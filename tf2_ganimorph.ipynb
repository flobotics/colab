{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2-ganimorph.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flobotics/colab/blob/master/tf2_ganimorph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3N_53xvtUQb",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/alpha/tutorials/generative/pix2pix\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/generative/dcgan\n",
        "\n",
        "https://www.tensorflow.org/alpha/tutorials/load_data/images#load_and_format_the_images\n",
        "\n",
        "https://www.tensorflow.org/guide/performance/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53NJP7dt9xT",
        "colab_type": "text"
      },
      "source": [
        "@inproceedings{Gokaslan2018,\n",
        "  title={Improving Shape Deformation in Unsupervised Image to Image Translation},\n",
        "  author={Aaron Gokaslan and Vivek Ramanujan and Daniel Ritchie and Kwang In Kim and James Tompkin},\n",
        "  booktitle={European Conference on Computer Vision},\n",
        "  year={2018}\n",
        "}\n",
        "\n",
        "https://github.com/brownvc/ganimorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAJoNuk5QHU2",
        "colab_type": "code",
        "outputId": "4ab2007e-3203-45a1-f369-92b09bbade13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXrMqG2hntC5",
        "colab_type": "code",
        "outputId": "e37bc819-0ea7-4bdf-de57-d8bf24811050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip3 install tensorflow-addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4FTapNoc7N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#from tensorpack import *\n",
        "#from tensorpack.utils.viz import *\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "#from tensorpack import (FeedfreeTrainerBase, QueueInput,\n",
        "#                        ModelDesc, DataFlow, StagingInputWrapper,\n",
        "#                        MultiGPUTrainerBase, LeastLoadedDeviceSetter)\n",
        "#from tensorpack.tfutils.summary import add_moving_summary\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "#import tensorpack.tfutils.symbolic_functions as symbf\n",
        "\n",
        "import cv2\n",
        "import os, sys\n",
        "import argparse\n",
        "from six.moves import map, zip\n",
        "from glob import glob\n",
        "\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#from tensorpack import *\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import IPython.display as display\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42GSJFUiZmZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpmUIEpKQSl",
        "colab_type": "text"
      },
      "source": [
        "# Input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcpyr6heK4C",
        "colab_type": "code",
        "outputId": "2ae4f4ea-0b87-4c56-d946-b3bd12878409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeSB753GeZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir /content/log\n",
        "#!cp -a /content/gdrive/My\\ Drive/images/model-11/* /content/log/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhjxVqpgH3M",
        "colab_type": "text"
      },
      "source": [
        "# Copy train and test data from e.g. Google-Drive to colab-notebook-machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaIQ_Lw5ebCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/data/\n",
        "\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/trainB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/trainB.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testA.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testA.tar.gz -C /content/data/\n",
        "\n",
        "!cp -a /content/gdrive/My\\ Drive/images/new-images/testB.tar.gz /content/data/\n",
        "!tar -xzf /content/data/testB.tar.gz -C /content/data/\n",
        "\n",
        "!rm /content/data/testA.tar.gz\n",
        "!rm /content/data/testB.tar.gz\n",
        "!rm /content/data/trainA.tar.gz\n",
        "!rm /content/data/trainB.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMABggQHRw0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAPE = 128\n",
        "#BATCH = 16\n",
        "TEST_BATCH = 32\n",
        "NF = 64  # channel size\n",
        "\n",
        "BATCH_SIZE  = 2\n",
        "#FLAGS.batch_size = BATCH\n",
        "#FLAGS.prefetch_buffer_size = BATCH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnqVhD777SF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_image(path):\n",
        "  print_debug = False\n",
        "  \n",
        "  image = tf.io.read_file(path)\n",
        "  \n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  if print_debug: print(\"original image-shape:{}\".format(image.shape))\n",
        "  if print_debug: print(\"original image-dtype:{}\".format(image.dtype))\n",
        "  \n",
        "  image = tf.image.resize(image, [SHAPE, SHAPE])\n",
        "  image /= 255.0  # normalize to [0,1] range\n",
        "  image = tf.transpose(image, perm=[2, 0, 1]) #seemed to break something, transpose ZipDataset\n",
        "\n",
        "  if print_debug: print(\"preprocessed image-shape:{}\".format(image.shape))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-min:{}\".format(image.numpy().min()))\n",
        "  #if print_debug: print(\"preprocessed image-numpy-max:{}\".format(image.numpy().max()))\n",
        "  \n",
        "  return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDEdSKrLXNF",
        "colab_type": "text"
      },
      "source": [
        "# Build Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7NAZUxrRZr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_debug = False\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "trainA_dir = \"/content/data/trainA\"\n",
        "trainB_dir = \"/content/data/trainB\"\n",
        "testA_dir = \"/content/data/testA\"\n",
        "testB_dir = \"/content/data/testB\"\n",
        "\n",
        "image_ds_trainA = tf.data.Dataset.list_files(trainA_dir+'/*.jpg')\n",
        "image_ds_trainA = image_ds_trainA.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainA = image_ds_trainA.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "image_ds_trainB = tf.data.Dataset.list_files(trainB_dir+'/*.jpg')\n",
        "image_ds_trainB = image_ds_trainB.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainB = image_ds_trainB.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "image_ds_testA = tf.data.Dataset.list_files(testA_dir+'/*.jpg')\n",
        "image_ds_testA = image_ds_testA.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testA = image_ds_testA.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "image_ds_testB = tf.data.Dataset.list_files(testB_dir+'/*.jpg')\n",
        "image_ds_testB = image_ds_testB.shuffle(SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testB = image_ds_testB.map(load_and_preprocess_image,\n",
        "                                  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "#--- Create ZipDataset\n",
        "image_ds_trainA_B = tf.data.Dataset.zip((image_ds_trainA, image_ds_trainB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_trainA_B))\n",
        "\n",
        "#--- Create ZipDataset\n",
        "image_ds_testA_B = tf.data.Dataset.zip((image_ds_testA, image_ds_testB))\n",
        "if print_debug: print(\"zipDataset  shapes/types:{}\".format(image_ds_testA_B))\n",
        "#print(\"len:{}\".format(image_ds_testA_B.))\n",
        "\n",
        "#------\n",
        "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
        "# completely shuffled.\n",
        "image_ds_trainA_B = image_ds_trainA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_trainA_B = image_ds_trainA_B.repeat()\n",
        "image_ds_trainA_B = image_ds_trainA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_trainA_B = image_ds_trainA_B.batch(BATCH_SIZE)\n",
        "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
        "image_ds_trainA_B = image_ds_trainA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_trainA_B shape/type {}\".format(image_ds_trainA_B))\n",
        "\n",
        "#test-images ds\n",
        "image_ds_testA_B = image_ds_testA_B.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.repeat()\n",
        "image_ds_testA_B = image_ds_testA_B.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=SHUFFLE_BUFFER_SIZE))\n",
        "image_ds_testA_B = image_ds_testA_B.batch(BATCH_SIZE)\n",
        "image_ds_testA_B = image_ds_testA_B.prefetch(buffer_size=AUTOTUNE)\n",
        "if print_debug: print(\"image_ds_testA_B shape/type {}\".format(image_ds_testA_B))\n",
        "\n",
        "  \n",
        "#for input_image, target in image_ds_testA_B:\n",
        "#  print(\"out:{} {}\".format(input_image, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y71bQ_un8bVz",
        "colab_type": "text"
      },
      "source": [
        "# Lets display some images from the train and test dataset to see if our dataset is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFr5gKqW4RFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  show_ds = image_ds_trainA_B.take(2)\n",
        "  print(show_ds)\n",
        "\n",
        "  for i1, i2 in show_ds:\n",
        "    plt.figure()\n",
        "    i1 = tf.transpose(i1, [0,2,3,1])\n",
        "    plt.imshow(i1[0,...])\n",
        "\n",
        "    plt.figure()\n",
        "    i2 = tf.transpose(i2, [0,2,3,1])\n",
        "    plt.imshow(i2[0,...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UlpAtDA8mE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  show_ds = image_ds_testA_B.take(2)\n",
        "  print(show_ds)\n",
        "\n",
        "  for i1, i2 in show_ds:\n",
        "    plt.figure()\n",
        "    i1 = tf.transpose(i1, [0,2,3,1])\n",
        "    plt.imshow(i1[0,...])\n",
        "\n",
        "    plt.figure()\n",
        "    i2 = tf.transpose(i2, [0,2,3,1])\n",
        "    plt.imshow(i2[0,...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNnanKKTMCo",
        "colab_type": "text"
      },
      "source": [
        "# build the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLpSzPzunVqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def INReLU(self,x, name=None):\n",
        "#  x = tfa.layers.InstanceNormalization()(x)\n",
        "#  x = layers.ReLU()(x)\n",
        "#  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTurwOdnnZhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def INLReLU(x, name=None):\n",
        "  #x = tfa.layers.InstanceNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONyQzQ6ndz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def BNLReLU(x, name):\n",
        "#    x = BatchNorm('bn', x)\n",
        "#    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHpB6pfI-Je1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = \"channels_first\"\n",
        "debug_show_shapes = 0\n",
        "\n",
        "\n",
        "def build_res_block(x, name, chan, first=False):\n",
        "  input = x\n",
        "  if debug_show_shapes: print(\"x-shape:{}\".format(x))\n",
        "\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(x)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  \n",
        "  l = tf.concat([l, input], axis=1)\n",
        "\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "  l = layers.Conv2D(filters=NF*2,\n",
        "                    kernel_size=3,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    data_format=df,\n",
        "                    kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                    use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"layer1: {}\".format(l.shape))\n",
        "  \n",
        "  return l\n",
        "\n",
        "  \n",
        "def res_group(input, name, depth, channels):\n",
        "  l = input\n",
        "  for k in range(depth):\n",
        "    l = build_res_block(l, name + ('/res%d' % k), channels,\n",
        "            first=(k==0))\n",
        "  return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPs8FP-Hle1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "\n",
        "  debug_show_shapes = False\n",
        "  subDepth = 3\n",
        "  df = \"channels_first\"\n",
        "  \n",
        "  inputs = tf.keras.Input(shape=(3, 128, 128)) # Returns a placeholder tensor\n",
        "  \n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  conv0 = layers.Conv2D(filters=NF, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv0: {}\".format(conv0.shape))\n",
        "  \n",
        "  conv1 = layers.Conv2D(filters=NF*2,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(conv0)\n",
        "\n",
        "  if debug_show_shapes: print(\"conv1: {}\".format(conv1.shape))\n",
        "  \n",
        "  #-------------------------\n",
        "  layer1 = res_group(conv1, 'layer1', subDepth, NF*2)\n",
        "  \n",
        "  #-----------------\n",
        "  conv2 = layers.Conv2D(filters=NF*4,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv2: {}\".format(conv2.shape))\n",
        "  #----------------\n",
        "  \n",
        "  layer2 = res_group(conv2, 'layer2', subDepth, NF*4)\n",
        "  \n",
        "  #-----------------\n",
        "  conv3 = layers.Conv2D(filters=NF*8,\n",
        "                        kernel_size=4,\n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        data_format=df,\n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                        use_bias=False,\n",
        "                        activation=INLReLU)(layer2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"conv3: {}\".format(conv3.shape))\n",
        "  #---------------\n",
        "  l = res_group(conv3, 'layer3', subDepth, NF*8)\n",
        "  \n",
        "  #--------------\n",
        "  \n",
        "  deconv0 = layers.Conv2DTranspose(filters=NF*4,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv0: {}\".format(deconv0.shape))\n",
        "  #-----------------\n",
        "  up1 = tf.concat([deconv0, layer2], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_2 = res_group(up1, 'blayer2', subDepth, NF * 4)\n",
        "  \n",
        "  #----------\n",
        "  deconv1 = layers.Conv2DTranspose(filters=NF*2,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv1: {}\".format(deconv1.shape))\n",
        "  \n",
        "  #-----------\n",
        "  up2 = tf.concat([deconv1, layer1], axis=1)\n",
        "  \n",
        "  #------------\n",
        "  b_layer_1 = res_group(up2, 'blayer1', subDepth, NF * 2)\n",
        "  \n",
        "  #----------\n",
        "  deconv2 = layers.Conv2DTranspose(filters=NF*1,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False,\n",
        "                                   activation=INLReLU)(b_layer_1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv2: {}\".format(deconv2.shape))\n",
        "  #-----------\n",
        "  deconv3 = layers.Conv2DTranspose(filters=3,\n",
        "                                   kernel_size=4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   data_format=df,\n",
        "                                   kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"),\n",
        "                                   use_bias=False, \n",
        "                                   activation=tf.sigmoid)(deconv2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"deconv3: {}\".format(deconv3.shape))\n",
        "  #-----------\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=deconv3)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvSfrg0mUISo",
        "colab_type": "code",
        "outputId": "0fa78b36-3fde-4a28-e16c-e3f050b51265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "generator = Generator()\n",
        "\n",
        "show_ds = image_ds_trainA_B.take(1)\n",
        "\n",
        "for i1, i2 in show_ds:\n",
        "  gen_output = generator(i1, training=False)\n",
        "\n",
        "gen_output = tf.transpose(gen_output, [0, 2, 3, 1])\n",
        "print(\"gen_output.shape: {}\".format(gen_output.shape))\n",
        "\n",
        "plt.imshow(gen_output[0,...])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gen_output.shape: (2, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f313239eba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGQRQzOfttj7",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvOoSsVaj03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  debug_show_shapes = 0\n",
        "  \n",
        "  subDepth = 3\n",
        "  df = \"channels_first\"\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(3, 128, 128))  # Returns a placeholder tensor\n",
        "  if debug_show_shapes: print(\"inputs: {}\".format(inputs.shape))\n",
        "  \n",
        "  l = layers.Conv2D(filters=NF*2, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=tf.nn.relu, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(inputs)\n",
        "  \n",
        "  if debug_show_shapes: print(\"l: {}\".format(l.shape))\n",
        "   \n",
        "\n",
        "  #-----------------\n",
        "  \n",
        "  relu1 = layers.Conv2D(filters=NF*4, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(l)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu1: {}\".format(relu1.shape))\n",
        "    \n",
        "  #-----------------\n",
        "    \n",
        "  relu2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=4, \n",
        "                        strides=2,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu1)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu2: {}\".format(relu2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  relu3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(relu2)\n",
        "  \n",
        "  if debug_show_shapes: print(\"relu3: {}\".format(relu3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "\n",
        "  \n",
        "  atrous = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=2,\n",
        "                        bias_initializer=None)(relu3)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous: {}\".format(atrous.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous2 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=4,\n",
        "                        bias_initializer=None)(atrous)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous2: {}\".format(atrous2.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  atrous3 = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False,\n",
        "                        dilation_rate=8,\n",
        "                        bias_initializer=None)(atrous2)\n",
        "  ##INLReLU\n",
        "  if debug_show_shapes: print(\"atrous3: {}\".format(atrous3.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  merge = tf.concat([relu3, atrous3], axis=1)\n",
        "  if debug_show_shapes: print(\"merge: {}\".format(merge.shape))\n",
        "  #-----------------\n",
        "  \n",
        "  clean = layers.Conv2D(filters=NF*8, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=INLReLU, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(merge)\n",
        "  \n",
        "  if debug_show_shapes: print(\"clean: {}\".format(clean.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  lsgan = layers.Conv2D(filters=1, \n",
        "                        kernel_size=3, \n",
        "                        strides=1,\n",
        "                        padding='same',\n",
        "                        activation=tf.identity, \n",
        "                        data_format=df, \n",
        "                        kernel_initializer=tf.initializers.VarianceScaling(scale=0.333, distribution=\"uniform\"), \n",
        "                        use_bias=False)(clean)\n",
        "  \n",
        "  if debug_show_shapes: print(\"lsgan: {}\".format(lsgan.shape))\n",
        "    \n",
        "  #-----------------\n",
        "  \n",
        "  model = tf.keras.Model(inputs=inputs, outputs=[lsgan, [relu1, relu2, relu3, atrous, atrous2, atrous3, clean]] )\n",
        "  \n",
        "  if debug_show_shapes: model.summary()\n",
        "\n",
        "  return model  #whats with the other outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qATmfX6bVDMh",
        "colab_type": "code",
        "outputId": "e6198f1e-81a0-4c35-a815-8adafccf98e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "discriminator = Discriminator()\n",
        "\n",
        "noise = tf.random.normal([1, 3, 128, 128])\n",
        "\n",
        "disc_out,a = discriminator([noise, gen_output], training=False)\n",
        "\n",
        "print(disc_out.shape)\n",
        "disc_out = tf.transpose(disc_out, [0, 2, 3, 1])\n",
        "print(disc_out.shape)\n",
        "\n",
        "#plt.imshow(disc_out[0,...])\n",
        "\n",
        "\n",
        "plt.imshow(disc_out[0,...,-1], cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 16, 16)\n",
            "(1, 16, 16, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f313217db00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAD8CAYAAAACGq0tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n50RoZoXnRGhmhedEaGaF919+Agb52WWKBAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxMw-xvK4Je",
        "colab_type": "text"
      },
      "source": [
        "# losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NCxP-CGYVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_match_loss(feats_real, feats_fake):\n",
        "\n",
        "  losses = []\n",
        "  for real, fake in zip(feats_real, feats_fake):\n",
        "      loss = tf.reduce_mean(tf.math.squared_difference(\n",
        "          tf.reduce_mean(real, 0),\n",
        "          tf.reduce_mean(fake, 0)),\n",
        "          name='mse_feat_' + real.op.name)\n",
        "      losses.append(loss)\n",
        "  ret = tf.reduce_mean(losses, name='feature_match_loss')\n",
        "  \n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_e9xIg3ClfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_value = tf.Variable(1.0, name='loss_scalar_val_',\n",
        "        trainable=False)\n",
        "\n",
        "loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_', trainable=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyHxPp0vGiCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_normalize(loss, update_condition, epsilon=1e-10):\n",
        "  \n",
        "  # Variable used for storing the scalar-value of the loss-function.\n",
        "  #loss_value = tf.Variable(1.0, name='loss_scalar_val_' + loss.op.name,\n",
        "  #      trainable=False)\n",
        "\n",
        "  #loss_value_smooth = (tf.Variable(1.0, name='loss_smooth_' +\n",
        "  #      loss.op.name, trainable=False))\n",
        "\n",
        "  #TODO don't update if is_training\n",
        "  ma_loss_value = (\n",
        "      moving_averages.assign_moving_average(\n",
        "              loss_value_smooth, loss, 0.9999, zero_debias=False, name='loss_EMA'\n",
        "          )\n",
        "      )\n",
        "\n",
        "  #tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ma_loss_value)\n",
        "  # Expression used for either updating the scalar-value or\n",
        "  # just re-using the old value.\n",
        "  # Note that when loss_value.assign(loss) is evaluated, it\n",
        "  # first evaluates the loss-function which is a TensorFlow\n",
        "  # expression, and then assigns the resulting scalar-value to\n",
        "  # the loss_value variable.\n",
        "  loss_value_updated = tf.cond(update_condition,\n",
        "                               lambda: loss_value.assign(ma_loss_value),\n",
        "                               lambda: loss_value)\n",
        "\n",
        "\n",
        "  # Expression for the normalized loss-function.\n",
        "  loss_normalized = loss / (loss_value_updated + epsilon)\n",
        "\n",
        "  #add_moving_summary(tf.identity(loss_value, name='loss_scalar_' + loss.op.name))\n",
        "\n",
        "  return loss_normalized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEUBZUZrRLg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _tf_fspecial_gauss(size, sigma):\n",
        "  \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
        "  \"\"\"\n",
        "  x_data, y_data = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
        "\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "  x_data = np.expand_dims(x_data, axis=-1)\n",
        "\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "  y_data = np.expand_dims(y_data, axis=-1)\n",
        "\n",
        "  x = tf.constant(x_data, dtype=tf.float32)\n",
        "  y = tf.constant(y_data, dtype=tf.float32)\n",
        "\n",
        "  g = tf.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
        "  return g / tf.reduce_sum(g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KXiarnhREpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ssim(img1, img2, cs_map=False, mean_metric=True, size=8, sigma=1.5):\n",
        "  window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
        "  K1 = 0.03\n",
        "  K2 = 0.05\n",
        "  L = 1  # depth of image (255 in case the image has a differnt scale)\n",
        "  C1 = (K1*L)**2\n",
        "  C2 = (K2*L)**2\n",
        "  mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1], padding='VALID')\n",
        "  mu1_sq = mu1*mu1\n",
        "  mu2_sq = mu2*mu2\n",
        "  mu1_mu2 = mu1*mu2\n",
        "  sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
        "  sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
        "  sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
        "  sigma1_sq = tf.abs(sigma1_sq)\n",
        "  sigma2_sq = tf.abs(sigma2_sq)\n",
        "  sigma12 = tf.abs(sigma12)\n",
        "  if cs_map:\n",
        "\n",
        "      value = (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2)),\n",
        "              (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
        "  else:\n",
        "      value = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
        "                  (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M2b0P0kQ9i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_ms_ssim(img1, img2, mean_metric=True, level=5):\n",
        "  #From NCHW to NHWC\n",
        "  img1 = tf.transpose(img1, [0, 2, 3, 1])\n",
        "  img2 = tf.transpose(img2, [0, 2, 3, 1])\n",
        "\n",
        "  weight = tf.constant([0.0448, 0.2856, 0.3001, 0.2363, 0.1333], dtype=tf.float32)\n",
        "  mssim = []\n",
        "  mcs = []\n",
        "  for l in range(level):\n",
        "      ssim_map, cs_map = tf_ssim(img1, img2, cs_map=True, mean_metric=False)\n",
        "      mssim.append(tf.reduce_mean(ssim_map))\n",
        "      mcs.append(tf.reduce_mean(cs_map))\n",
        "      filtered_im1 = tf.nn.avg_pool(img1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      filtered_im2 = tf.nn.avg_pool(img2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
        "      img1 = filtered_im1\n",
        "      img2 = filtered_im2\n",
        "\n",
        "  # list to tensor of dim D+1\n",
        "  mssim = tf.stack(mssim, axis=0)\n",
        "  mcs = tf.stack(mcs, axis=0)\n",
        "\n",
        "  value = (tf.reduce_prod(mcs[0:level-1]**weight[0:level-1])*\n",
        "                          (mssim[level-1]**weight[level-1]))\n",
        "\n",
        "  if mean_metric:\n",
        "      value = tf.reduce_mean(value)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF5iWLVAQrh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_dssim(img1, img2):\n",
        "  img1 = tf.unstack(tf.expand_dims(img1, axis=2), axis=1)\n",
        "  img2 = tf.unstack(tf.expand_dims(img2, axis=2), axis=1)\n",
        "  value = tf.stack([tf_ms_ssim(i1, i2) for i1, i2 in zip(img1, img2)], axis=0)\n",
        "  return tf.subtract(1.0, tf.reduce_sum(value)/3, name='DSSIM_loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErsvNLhPTGqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_losses(logits_real, logits_fake):\n",
        "  #Build standard GAN loss and set `self.g_loss` and `self.d_loss`.\n",
        "  #D and G play two-player minimax game with value function V(G,D)\n",
        "  #  min_G max _D V(D, G) = IE_{x ~ p_data} [log D(x)] + IE_{z ~ p_fake} [log (1 - D(G(z)))]\n",
        "  #Args:\n",
        "  #    logits_real (tf.Tensor): discrim logits from real samples\n",
        "  #    logits_fake (tf.Tensor): discrim logits from fake samples produced by generator\n",
        "\n",
        "  with tf.name_scope(\"GAN_loss\"):\n",
        "    score_real = tf.sigmoid(logits_real)\n",
        "    score_fake = tf.sigmoid(logits_fake)\n",
        "    #tf.summary.histogram('score-real', score_real)\n",
        "    #tf.summary.histogram('score-fake', score_fake)\n",
        "\n",
        "    with tf.name_scope(\"discrim\"):\n",
        "        d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n",
        "        d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n",
        "\n",
        "        d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n",
        "        d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n",
        "\n",
        "        d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n",
        "        d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n",
        "\n",
        "    with tf.name_scope(\"gen\"):\n",
        "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n",
        "        g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n",
        "\n",
        "    #add_moving_summary(g_loss, d_loss, d_accuracy, g_accuracy)\n",
        "    \n",
        "    return g_loss, d_loss, d_accuracy, g_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpT89fEdmnHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "  \n",
        "  return d_loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KepfzD6nm2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake):\n",
        "  print_debug = False\n",
        "  \n",
        "  recon_loss_A = tf_dssim(A, ABA)\n",
        "  if print_debug: print(\"recon_loss_A-shape: {}\".format(recon_loss_A.shape))\n",
        "  if print_debug: print(\"A-shape: {}\".format(A.shape))\n",
        "  if print_debug: print(\"ABA-shape: {}\".format(ABA.shape))\n",
        "  recon_loss_A_l = tf.compat.v1.losses.absolute_difference(A,\n",
        "                                            ABA,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_A_l-shape: {}\".format(recon_loss_A_l.shape))\n",
        "\n",
        "  # gan loss\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(A_dis_real, A_dis_fake)\n",
        "  G_loss_A = g_loss\n",
        "  D_loss_A = d_loss\n",
        "  # feature matching loss\n",
        "  if print_debug: print(A_feats_fake)\n",
        "  fm_loss_A = get_feature_match_loss(A_feats_real, A_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  recon_loss_B = tf_dssim(B, BAB)\n",
        "  if print_debug: print(\"recon_loss_B-shape: {}\".format(recon_loss_B.shape))\n",
        "\n",
        "  recon_loss_B_l = tf.compat.v1.losses.absolute_difference(B,\n",
        "                                            BAB,\n",
        "                                            reduction=tf.compat.v1.losses.Reduction.MEAN)\n",
        "  if print_debug: print(\"recon_loss_B_l-shape: {}\".format(recon_loss_B_l.shape))\n",
        "\n",
        "  g_loss, d_loss, d_accuracy, g_accuracy = build_losses(B_dis_real, B_dis_fake)\n",
        "  G_loss_B = g_loss\n",
        "  D_loss_B = d_loss# + grad_penalty_B\n",
        "  fm_loss_B = get_feature_match_loss(B_feats_real, B_feats_fake)\n",
        "  #-------------------\n",
        "\n",
        "  global_step = np.int64(1)   #get_global_step_var()\n",
        "  rate = 0.01  #tf.train.piecewise_constant(global_step, [np.int64(15000), np.int64(25000), np.int64(50000), np.int64(100000)], [0.01, 0.10, 0.15, 0.20, 0.25])\n",
        "  #rate = tf.identity(rate, name='rate')   # mitigate a TF bug\n",
        "  loss_update = tf.logical_or(tf.equal(global_step, tf.constant(36,\n",
        "      dtype=np.int64)), tf.equal(global_step % 90, tf.constant(0, dtype=np.int64)))\n",
        "  rate = tf.constant(0.33, np.float32, name='static_rate')\n",
        "\n",
        "\n",
        "  g_loss = tf.add_n([\n",
        "      (loss_normalize(G_loss_A + G_loss_B, loss_update) * 0.7 +\n",
        "      loss_normalize(fm_loss_A + fm_loss_B, loss_update) * 0.3) * (1 - rate),\n",
        "      (loss_normalize((recon_loss_A + recon_loss_B), loss_update) *\n",
        "          0.7 +\n",
        "  loss_normalize((recon_loss_A_l + recon_loss_B_l),\n",
        "              loss_update) * 0.3) * rate], name='G_loss_total')\n",
        "\n",
        "\n",
        "  d_loss = tf.add(D_loss_A, D_loss_B, name='D_loss_total')\n",
        "\n",
        "  #@TODO what todo? collect_variables('gen', 'discrim')\n",
        "\n",
        "  g_loss = g_loss\n",
        "  d_loss = d_loss \n",
        "\n",
        "  \n",
        "  return g_loss, d_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-9Z01QLVjF",
        "colab_type": "text"
      },
      "source": [
        "# optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk036iXixrHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lr = tf.get_variable('learning_rate', initializer=2e-4,\n",
        "#                 trainable=False)\n",
        "#        return tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHsakYaLhz9",
        "colab_type": "text"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKNKgCD5Vzey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_variables(self, g_scope='gen', d_scope='discrim'):\n",
        "  #\"\"\"\n",
        "  #Assign `self.g_vars` to the parameters under scope `g_scope`,\n",
        "  #and same with `self.d_vars`.\n",
        "  #\"\"\"\n",
        "  self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n",
        "  assert self.g_vars\n",
        "  self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n",
        "  assert self.d_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQatM82Ty-EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A = tf.Variable(1.0)\n",
        "#AB = tf.Variable([3, 128, 128])\n",
        "#AB = None\n",
        "#BA = None\n",
        "#ABA = None\n",
        "#BAB = None\n",
        "\n",
        "print_debug = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdQWnkOGx4gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(A, B):\n",
        "#  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "#    gen_output = generator(input_image, training=True)\n",
        "#\n",
        "#    disc_real_output = discriminator([input_image, target], training=True)\n",
        "#    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "#\n",
        "#    gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "#    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "#  generator_gradients = gen_tape.gradient(gen_loss,\n",
        "#                                          generator.trainable_variables)\n",
        "#  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "#                                               discriminator.trainable_variables)\n",
        "\n",
        "#  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "#                                          generator.trainable_variables))\n",
        "#  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "#                                              discriminator.trainable_variables))\n",
        "  \n",
        "  \n",
        "  if print_debug: print(\"train step\")\n",
        "  \n",
        "  print(\"train\")\n",
        "\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    AB = generator(A, training=True)\n",
        "    BA = generator(B, training=True)\n",
        "    ABA = generator(AB, training=True)\n",
        "    BAB = generator(BA, training=True)\n",
        "    if print_debug: print(\"BAB\")\n",
        "    \n",
        "    A_dis_real, A_feats_real = discriminator(A, training=True)\n",
        "    A_dis_fake, A_feats_fake = discriminator(BA, training=True)\n",
        "    B_dis_real, B_feats_real = discriminator(B, training=True)\n",
        "    B_dis_fake, B_feats_fake = discriminator(AB, training=True)\n",
        "  \n",
        "    #------------\n",
        "    g_loss, d_loss = generator_loss(A, ABA, B, BAB, A_dis_real, A_feats_real,\n",
        "                                   A_dis_fake, A_feats_fake,\n",
        "                                   B_dis_real, B_feats_real,\n",
        "                                   B_dis_fake, B_feats_fake)\n",
        "      \n",
        "  gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))  \n",
        "    \n",
        "  if print_debug: print(\"end train step\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbP4P_KFxtph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BocGsTuOL8BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = tf.random.normal([1, 3, 128, 128])\n",
        "dstest = image_ds_testA_B.take(1)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  print(\"generate_and_save_images\")\n",
        "  \n",
        "  for input_image, target in dstest:\n",
        "  \n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(input_image, training=False)\n",
        "\n",
        "  predictions = tf.transpose(predictions, [0, 2, 3, 1])\n",
        "  print(\"predications shape:{}\".format(predictions.shape))\n",
        "  #plt.imshow(predictions[0,...])\n",
        "  \n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i,...])\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0iKglUyVOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_image, target in dataset:\n",
        "      train_step(input_image, target)\n",
        "      \n",
        "    clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             1,\n",
        "                             seed)\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngXLySdSydzQ",
        "colab_type": "code",
        "outputId": "9054f3e7-4150-4b5e-ffed-26d3bf9fbee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "EPOCHS = 200\n",
        "print(image_ds_trainA_B)\n",
        "\n",
        "dst = image_ds_trainA_B.take(10)\n",
        "\n",
        "train(dst, EPOCHS)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generate_and_save_images\n",
            "predications shape:(2, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJEAAABSCAYAAABUpDY5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAP9JREFUeJzt3TEKQyEQQEENOfje3Fwh8Iofw0xt\nscJjGwv3OWdB8Xp6AO4nIjIRkYmITERkIiITEZmIyEREJiKy99MDrLXWzFz99jIz+8tzf3lPm4hM\nRGQiIhMRmYjIREQmIjIRkYmITERkIiITEZmIyEREJiIyEZGJiExEZCIiExGZiMhERCYiMhGRiYhM\nRGQiIhMRmYjIREQmIjIRkYmITERkIiITEZmIyEREJiIyEZGJiExEZCIiExGZiMhERLbPufr3JH6A\nTUQmIjIRkYmITERkIiITEZmIyEREJiIyEZGJiExEZCIiExGZiMhERCYiMhGRiYhMRGQiIhMR2Qe1\ntw6fL1/fUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 2 is 6.427367448806763 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-8643d2ce0951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_ds_trainA_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-6cc85132cb51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1941\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}